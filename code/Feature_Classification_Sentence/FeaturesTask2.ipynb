{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeaturesTask2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "d_qscvx56qIP",
        "colab_type": "code",
        "outputId": "3437fc94-aaa3-4ef1-9595-c4b991478c34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install textstat\n",
        "\n",
        "\n",
        "# Imports and downloads\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf \n",
        "from google.colab import drive\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from scipy.sparse import hstack # to concatenate features\n",
        "from sklearn.svm import LinearSVC\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import sentiwordnet as swn, wordnet\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from textblob import TextBlob\n",
        "import textstat\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "from gensim.models import KeyedVectors\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "# Read the data + emotions\n",
        "df = pd.read_pickle('/content/gdrive/My Drive/datathon/task2data.pkl')\n",
        "\n",
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.6/dist-packages (from textstat) (0.9.5)\n",
            "Requirement already satisfied: repoze.lru in /usr/local/lib/python3.6/dist-packages (from textstat) (0.7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Found GPU at: /device:GPU:0\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>N_sentence</th>\n",
              "      <th>sadness</th>\n",
              "      <th>joy</th>\n",
              "      <th>fear</th>\n",
              "      <th>disgust</th>\n",
              "      <th>anger</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.426300e+04</td>\n",
              "      <td>14263.000000</td>\n",
              "      <td>14263.000000</td>\n",
              "      <td>14263.000000</td>\n",
              "      <td>14263.000000</td>\n",
              "      <td>14263.000000</td>\n",
              "      <td>14263.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>9.440644e+08</td>\n",
              "      <td>47.675384</td>\n",
              "      <td>0.257924</td>\n",
              "      <td>0.202204</td>\n",
              "      <td>0.132644</td>\n",
              "      <td>0.171051</td>\n",
              "      <td>0.168878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.145074e+09</td>\n",
              "      <td>55.272256</td>\n",
              "      <td>0.177576</td>\n",
              "      <td>0.192200</td>\n",
              "      <td>0.131431</td>\n",
              "      <td>0.160697</td>\n",
              "      <td>0.143378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.111111e+08</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.289730e+08</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>0.127318</td>\n",
              "      <td>0.052781</td>\n",
              "      <td>0.049964</td>\n",
              "      <td>0.056007</td>\n",
              "      <td>0.067621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>7.618971e+08</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.222080</td>\n",
              "      <td>0.143389</td>\n",
              "      <td>0.095208</td>\n",
              "      <td>0.116630</td>\n",
              "      <td>0.127791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>7.820171e+08</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>0.359406</td>\n",
              "      <td>0.295384</td>\n",
              "      <td>0.169027</td>\n",
              "      <td>0.241330</td>\n",
              "      <td>0.231704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.709564e+09</td>\n",
              "      <td>429.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.931034</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            article    N_sentence       sadness           joy          fear  \\\n",
              "count  1.426300e+04  14263.000000  14263.000000  14263.000000  14263.000000   \n",
              "mean   9.440644e+08     47.675384      0.257924      0.202204      0.132644   \n",
              "std    1.145074e+09     55.272256      0.177576      0.192200      0.131431   \n",
              "min    1.111111e+08      1.000000      0.000000      0.000000      0.000000   \n",
              "25%    7.289730e+08     14.000000      0.127318      0.052781      0.049964   \n",
              "50%    7.618971e+08     29.000000      0.222080      0.143389      0.095208   \n",
              "75%    7.820171e+08     58.000000      0.359406      0.295384      0.169027   \n",
              "max    7.709564e+09    429.000000      1.000000      1.000000      1.000000   \n",
              "\n",
              "            disgust         anger  \n",
              "count  14263.000000  14263.000000  \n",
              "mean       0.171051      0.168878  \n",
              "std        0.160697      0.143378  \n",
              "min        0.000000      0.000000  \n",
              "25%        0.056007      0.067621  \n",
              "50%        0.116630      0.127791  \n",
              "75%        0.241330      0.231704  \n",
              "max        0.931034      1.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "mJNS4B988QmY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Total of 14263 sentences."
      ]
    },
    {
      "metadata": {
        "id": "R4jxK0Za693a",
        "colab_type": "code",
        "outputId": "16558372-a823-45b2-d2a9-c6e79dffcdfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>article</th>\n",
              "      <th>N_sentence</th>\n",
              "      <th>is_propaganda</th>\n",
              "      <th>sadness</th>\n",
              "      <th>joy</th>\n",
              "      <th>fear</th>\n",
              "      <th>disgust</th>\n",
              "      <th>anger</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>New Audio From The Night Of The Las Vegas Mass...</td>\n",
              "      <td>704856340</td>\n",
              "      <td>1</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.093891</td>\n",
              "      <td>0.481352</td>\n",
              "      <td>0.238193</td>\n",
              "      <td>0.133129</td>\n",
              "      <td>0.162108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Newly released audio from the Clark County Fir...</td>\n",
              "      <td>704856340</td>\n",
              "      <td>3</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.456951</td>\n",
              "      <td>0.024205</td>\n",
              "      <td>0.162442</td>\n",
              "      <td>0.419307</td>\n",
              "      <td>0.281755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The audio, released on the SoundCloud account ...</td>\n",
              "      <td>704856340</td>\n",
              "      <td>4</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.193504</td>\n",
              "      <td>0.280196</td>\n",
              "      <td>0.125014</td>\n",
              "      <td>0.140594</td>\n",
              "      <td>0.366930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Intellihub’s Shepard Ambellas, who has extensi...</td>\n",
              "      <td>704856340</td>\n",
              "      <td>5</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.674633</td>\n",
              "      <td>0.155427</td>\n",
              "      <td>0.046534</td>\n",
              "      <td>0.116447</td>\n",
              "      <td>0.051066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>“We have a firefighter’s wife at this event wh...</td>\n",
              "      <td>704856340</td>\n",
              "      <td>6</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.356464</td>\n",
              "      <td>0.171645</td>\n",
              "      <td>0.098791</td>\n",
              "      <td>0.098959</td>\n",
              "      <td>0.272835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>We are trying to get further on the name,” dis...</td>\n",
              "      <td>704856340</td>\n",
              "      <td>7</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.395951</td>\n",
              "      <td>0.058361</td>\n",
              "      <td>0.141609</td>\n",
              "      <td>0.075802</td>\n",
              "      <td>0.107938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>“Batallion 6, be advised that we are getting r...</td>\n",
              "      <td>704856340</td>\n",
              "      <td>8</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.082155</td>\n",
              "      <td>0.041201</td>\n",
              "      <td>0.110452</td>\n",
              "      <td>0.028624</td>\n",
              "      <td>0.071853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>After being asked to confirm the information, ...</td>\n",
              "      <td>704856340</td>\n",
              "      <td>9</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.457318</td>\n",
              "      <td>0.065556</td>\n",
              "      <td>0.055998</td>\n",
              "      <td>0.069855</td>\n",
              "      <td>0.213325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>“The only information I have is it’s the bar o...</td>\n",
              "      <td>704856340</td>\n",
              "      <td>10</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.477513</td>\n",
              "      <td>0.157989</td>\n",
              "      <td>0.067258</td>\n",
              "      <td>0.036316</td>\n",
              "      <td>0.191752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Interestingly, the story doesn’t end there, as...</td>\n",
              "      <td>704856340</td>\n",
              "      <td>11</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.109102</td>\n",
              "      <td>0.687029</td>\n",
              "      <td>0.089832</td>\n",
              "      <td>0.021488</td>\n",
              "      <td>0.131763</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentences    article  N_sentence  \\\n",
              "0   New Audio From The Night Of The Las Vegas Mass...  704856340           1   \n",
              "2   Newly released audio from the Clark County Fir...  704856340           3   \n",
              "3   The audio, released on the SoundCloud account ...  704856340           4   \n",
              "4   Intellihub’s Shepard Ambellas, who has extensi...  704856340           5   \n",
              "5   “We have a firefighter’s wife at this event wh...  704856340           6   \n",
              "6   We are trying to get further on the name,” dis...  704856340           7   \n",
              "7   “Batallion 6, be advised that we are getting r...  704856340           8   \n",
              "8   After being asked to confirm the information, ...  704856340           9   \n",
              "9   “The only information I have is it’s the bar o...  704856340          10   \n",
              "10  Interestingly, the story doesn’t end there, as...  704856340          11   \n",
              "\n",
              "     is_propaganda   sadness       joy      fear   disgust     anger  \n",
              "0   non-propaganda  0.093891  0.481352  0.238193  0.133129  0.162108  \n",
              "2   non-propaganda  0.456951  0.024205  0.162442  0.419307  0.281755  \n",
              "3   non-propaganda  0.193504  0.280196  0.125014  0.140594  0.366930  \n",
              "4   non-propaganda  0.674633  0.155427  0.046534  0.116447  0.051066  \n",
              "5   non-propaganda  0.356464  0.171645  0.098791  0.098959  0.272835  \n",
              "6   non-propaganda  0.395951  0.058361  0.141609  0.075802  0.107938  \n",
              "7   non-propaganda  0.082155  0.041201  0.110452  0.028624  0.071853  \n",
              "8   non-propaganda  0.457318  0.065556  0.055998  0.069855  0.213325  \n",
              "9   non-propaganda  0.477513  0.157989  0.067258  0.036316  0.191752  \n",
              "10  non-propaganda  0.109102  0.687029  0.089832  0.021488  0.131763  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "q-M3ErY27gkB",
        "colab_type": "code",
        "outputId": "ffbb4256-d333-4390-de9b-e00ccf52a693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(df.loc[df['is_propaganda'] == 'propaganda'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "mBGZdu8j8iMl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3938 of those are propaganda."
      ]
    },
    {
      "metadata": {
        "id": "5lahbhRg5nq4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "metadata": {
        "id": "wThynRQHGRvQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Number of words\n",
        "def get_num_words(text):\n",
        "    \n",
        "    # Remove special chars and punctuation\n",
        "    tokens = text.split()\n",
        "    \n",
        "    return len(tokens)\n",
        "\n",
        "# Num of chars\n",
        "def get_num_chars(text):    \n",
        "    return len(text)\n",
        "  \n",
        "df['num_words'] = df['sentences'].apply(get_num_words)\n",
        "df['num_chars'] = df['sentences'].apply(get_num_chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ANRwYV6cLtBJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def count_adjectives(text):\n",
        "    num_adjs = 0\n",
        "    tags = pos_tag(word_tokenize(text.lower()))\n",
        "    for t in tags:\n",
        "      if t[1].startswith('JJ'):\n",
        "        num_adjs += 1\n",
        "    return num_adjs\n",
        "  \n",
        "  \n",
        "def count_adverbs(text):\n",
        "    num_adv = 0\n",
        "    tags = pos_tag(word_tokenize(text.lower()))\n",
        "    for t in tags:\n",
        "      if t[1].startswith('RB'):\n",
        "        num_adv += 1\n",
        "    return num_adv\n",
        "  \n",
        "def count_singular_pronouns(text):\n",
        "    \n",
        "    sing_pro = 0\n",
        "    tags = pos_tag(word_tokenize(text.lower()))\n",
        "    for t in tags:\n",
        "      if t[1].startswith('NNP'):\n",
        "        sing_pro += 1\n",
        "    return sing_pro\n",
        "  \n",
        "  \n",
        "def count_plural_pronouns(text):\n",
        "  \n",
        "    plur_pro = 0\n",
        "    tags = pos_tag(word_tokenize(text.lower()))\n",
        "    for t in tags:\n",
        "      if t[1].startswith('NNP'):\n",
        "        plur_pro += 1\n",
        "    return plur_pro\n",
        "  \n",
        "df['num_adjs'] = df['sentences'].apply(count_adjectives)\n",
        "df['num_adv'] = df['sentences'].apply(count_adverbs)\n",
        "df['sing_pro'] = df['sentences'].apply(count_singular_pronouns)\n",
        "df['pl_pro'] = df['sentences'].apply(count_plural_pronouns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rw78n1t-Q8FA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def is_question(sentence):\n",
        "    if len(sentence) == 0:\n",
        "        return 0\n",
        "    return 1 if sentence[len(sentence) - 1] == '?' else 0\n",
        "\n",
        "def is_exclamation(sentence):\n",
        "    if len(sentence) == 0:\n",
        "        return 0\n",
        "    return 1 if sentence[len(sentence) - 1] == '!' else 0\n",
        "\n",
        "def is_period(sentence):\n",
        "    if len(sentence) == 0:\n",
        "        return 0\n",
        "    return 1 if sentence[len(sentence) - 1] == '.' else 0\n",
        "  \n",
        "df['is_question'] = df['sentences'].apply(is_question)\n",
        "df['is_exclamation'] = df['sentences'].apply(is_exclamation)\n",
        "df['is_period'] = df['sentences'].apply(is_period)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ctFyjbfS6nO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Polarity feat\n",
        "\n",
        "def polarity(sentence):\n",
        "    \"\"\"\n",
        "    :param sentences list of sentences\n",
        "    :return: a list of singleton lists which contain\n",
        "             a number between -1 and 1,\n",
        "             where -1 is negative and 1 is positive.\n",
        "    \"\"\"\n",
        "    return TextBlob(sentence).sentiment.polarity\n",
        "\n",
        "df['polarity'] = df['sentences'].apply(polarity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7UHHQq7dTYhQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Subjectivity feat\n",
        "\n",
        "def subjectivity(sentence):\n",
        "    \"\"\"\n",
        "    :param sentences list of sentences\n",
        "    :return: a list of singleton lists which contain\n",
        "            a number between 0 and 1,\n",
        "            where 0 is objective and 1 is subjective.\n",
        "    \"\"\"\n",
        "    return TextBlob(sentence).sentiment.subjectivity\n",
        "\n",
        "df['subjectivity'] = df['sentences'].apply(subjectivity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AcKUO2t5VNxu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Readability features\n",
        "\n",
        "df['flesch_reading_ease'] = df['sentences'].apply(textstat.flesch_reading_ease)\n",
        "df['smog_index'] = df['sentences'].apply(textstat.smog_index)\n",
        "df['flesch_kincaid_grade'] = df['sentences'].apply(textstat.flesch_kincaid_grade)\n",
        "df['coleman_liau_index'] = df['sentences'].apply(textstat.coleman_liau_index)\n",
        "df['automated_readability_index'] = df['sentences'].apply(textstat.automated_readability_index)\n",
        "df['dale_chall_readability_score'] = df['sentences'].apply(textstat.dale_chall_readability_score)\n",
        "df['difficult_words'] = df['sentences'].apply(textstat.difficult_words)\n",
        "df['linsear_write_formula'] = df['sentences'].apply(textstat.linsear_write_formula)\n",
        "df['gunning_fog'] = df['sentences'].apply(textstat.gunning_fog)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uf-Sc00QjdRI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lexical_chars(chars, key_wanted):\n",
        "    char_count = len(chars)\n",
        "\n",
        "    possible_chars_map = {\n",
        "        ',': 'comma_count',\n",
        "        '\\n': 'paragraph_count',\n",
        "        ';': 'semicolon_count',\n",
        "        ':': 'colon_count',\n",
        "        ' ': 'spaces_count',\n",
        "        '\\'': 'apostrophes_count',\n",
        "        '&': 'amp_count'\n",
        "    }\n",
        "\n",
        "    possible_chars = possible_chars_map.keys()\n",
        "\n",
        "    char_analysis = {\n",
        "        'digits': 0,\n",
        "        'punctuation_count': 0,\n",
        "        'comma_count': 0,\n",
        "        'semicolon_count': 0,\n",
        "        'colon_count': 0,\n",
        "        'spaces_count': 0,\n",
        "        'apostrophes_count': 0,\n",
        "        'amp_count': 0,\n",
        "        'parenthesis_count': 0,\n",
        "        'paragraph_count': 0\n",
        "    }\n",
        "\n",
        "    for char in chars:\n",
        "        if char in possible_chars:\n",
        "            char_analysis[possible_chars_map[char]] += 1\n",
        "        elif char.isdigit(): char_analysis['digits'] += 1\n",
        "        elif char in ['(', ')']: char_analysis['parenthesis_count'] += 1\n",
        "        if char in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~': char_analysis['punctuation_count'] += 1\n",
        "\n",
        "    return char_analysis[key_wanted]/char_count\n",
        "\n",
        "df['digits'] = df['sentences'].apply(lambda x: lexical_chars(x, 'digits'))\n",
        "df['punctuation_count'] = df['sentences'].apply(lambda x: lexical_chars(x, 'punctuation_count'))\n",
        "df['comma_count'] = df['sentences'].apply(lambda x: lexical_chars(x, 'comma_count'))\n",
        "df['semicolon_count'] = df['sentences'].apply(lambda x: lexical_chars(x, 'semicolon_count'))\n",
        "df['colon_count'] = df['sentences'].apply(lambda x: lexical_chars(x, 'colon_count'))\n",
        "df['spaces_count'] = df['sentences'].apply(lambda x: lexical_chars(x, 'spaces_count'))\n",
        "df['apostrophes_count'] = df['sentences'].apply(lambda x: lexical_chars(x, 'apostrophes_count'))\n",
        "df['amp_count'] = df['sentences'].apply(lambda x: lexical_chars(x, 'amp_count'))\n",
        "df['parenthesis_count'] = df['sentences'].apply(lambda x: lexical_chars(x, 'parenthesis_count'))\n",
        "df['paragraph_count'] = df['sentences'].apply(lambda x: lexical_chars(x, 'paragraph_count'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RRn5Yc3YnqmL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lexical_words(sent, key_wanted):\n",
        "    \n",
        "    word_count = len(sent)\n",
        "    entry_word = word_tokenize(sent)\n",
        "    words_tagged = pos_tag(entry_word)\n",
        "    \n",
        "    word_analysis = {\n",
        "        'pronouns': 0,\n",
        "        'prepositions': 0,\n",
        "        'coordinating_conjunctions': 0,\n",
        "        'adjectives': 0,\n",
        "        'adverbs': 0,\n",
        "        'determiners': 0,\n",
        "        'interjections': 0,\n",
        "        'modals': 0,\n",
        "        'nouns': 0,\n",
        "        'personal_pronouns': 0,\n",
        "        'verbs': 0,\n",
        "        'word_len_gte_six': 0,\n",
        "        'word_len_two_and_three': 0,\n",
        "        'avg_word_length': 0,\n",
        "        'all_caps': 0,\n",
        "        'capitalized': 0,\n",
        "        'quotes_count': 0,\n",
        "    }\n",
        "\n",
        "    for (word, tag) in words_tagged:\n",
        "        if tag in ['PRP']: word_analysis['personal_pronouns'] += 1\n",
        "        if tag.startswith('J'): word_analysis['adjectives'] += 1\n",
        "        if tag.startswith('N'): word_analysis['nouns'] += 1\n",
        "        if tag.startswith('V'): word_analysis['verbs'] += 1\n",
        "        if tag in ['PRP', 'PRP$', 'WP', 'WP$']: word_analysis['pronouns'] += 1\n",
        "        elif tag in ['IN']: word_analysis['prepositions'] += 1\n",
        "        elif tag in ['CC']: word_analysis['coordinating_conjunctions'] += 1\n",
        "        elif tag in ['RB', 'RBR', 'RBS']: word_analysis['adverbs'] += 1\n",
        "        elif tag in ['DT', 'PDT', 'WDT']: word_analysis['determiners'] += 1\n",
        "        elif tag in ['UH']: word_analysis['interjections'] += 1\n",
        "        elif tag in ['MD']: word_analysis['modals'] += 1\n",
        "        if len(word) >= 6: word_analysis['word_len_gte_six'] += 1\n",
        "        elif len(word) in [2, 3]: word_analysis['word_len_two_and_three'] += 1\n",
        "        word_analysis['avg_word_length'] += len(word)\n",
        "        if word.isupper(): word_analysis['all_caps'] += 1\n",
        "        elif word[0].isupper(): word_analysis['capitalized'] += 1\n",
        "        word_analysis['quotes_count'] += word.count('\"') + word.count('`') + word.count('\\'')\n",
        "\n",
        "    return word_analysis[key_wanted]/word_count\n",
        "\n",
        "df['pronouns'] = df['sentences'].apply(lambda x: lexical_words(x, 'pronouns'))\n",
        "df['prepositions'] = df['sentences'].apply(lambda x: lexical_words(x, 'prepositions'))\n",
        "df['coordinating_conjunctions'] = df['sentences'].apply(lambda x: lexical_words(x, 'coordinating_conjunctions'))\n",
        "df['adjectives'] = df['sentences'].apply(lambda x: lexical_words(x, 'adjectives'))\n",
        "df['adverbs'] = df['sentences'].apply(lambda x: lexical_words(x, 'adverbs'))\n",
        "df['determiners'] = df['sentences'].apply(lambda x: lexical_words(x, 'determiners'))\n",
        "df['interjections'] = df['sentences'].apply(lambda x: lexical_words(x, 'interjections'))\n",
        "df['modals'] = df['sentences'].apply(lambda x: lexical_words(x, 'modals'))\n",
        "df['nouns'] = df['sentences'].apply(lambda x: lexical_words(x, 'nouns'))\n",
        "df['personal_pronouns'] = df['sentences'].apply(lambda x: lexical_words(x, 'personal_pronouns'))\n",
        "df['verbs'] = df['sentences'].apply(lambda x: lexical_words(x, 'verbs'))\n",
        "df['word_len_gte_six'] = df['sentences'].apply(lambda x: lexical_words(x, 'word_len_gte_six'))\n",
        "df['word_len_two_and_three'] = df['sentences'].apply(lambda x: lexical_words(x, 'word_len_two_and_three'))\n",
        "df['avg_word_length'] = df['sentences'].apply(lambda x: lexical_words(x, 'avg_word_length'))\n",
        "df['all_caps'] = df['sentences'].apply(lambda x: lexical_words(x, 'all_caps'))\n",
        "df['capitalized'] = df['sentences'].apply(lambda x: lexical_words(x, 'capitalized'))\n",
        "df['quotes_count'] = df['sentences'].apply(lambda x: lexical_words(x, 'quotes_count'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XGDj2veWw9ua",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_score(sentence):\n",
        "    sent_score = []\n",
        "    words = pos_tag(sentence.split())\n",
        "    for t in words:\n",
        "        word = t[0]\n",
        "        tag = t[1]\n",
        "        new_tag = ''\n",
        "        if tag.startswith('NN'):\n",
        "            new_tag = wordnet.NOUN\n",
        "        elif tag.startswith('J'):\n",
        "            new_tag = wordnet.ADJ\n",
        "        elif tag.startswith('V'):\n",
        "            new_tag = wordnet.VERB\n",
        "        elif tag.startswith('R'):\n",
        "            new_tag = wordnet.ADV\n",
        "\n",
        "        if new_tag != '':\n",
        "            synsets = list(swn.senti_synsets(word, new_tag))\n",
        "            score = 0.0\n",
        "            if len(synsets) > 0:\n",
        "                for syn in synsets:\n",
        "                    score += syn.pos_score() - syn.neg_score()\n",
        "                sent_score.append(score / len(synsets))\n",
        "\n",
        "    if len(sent_score)==0:\n",
        "        return float(0.0)\n",
        "    else:\n",
        "        return np.mean(sent_score)\n",
        "\n",
        "df['sentiment_score'] = df['sentences'].apply(compute_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ys4pFYkK0RS-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalized_number_meanings(sentence):\n",
        "    transformed = []\n",
        "    tokens = word_tokenize(sentence)\n",
        "    words_tagged = pos_tag(tokens)\n",
        "    word_vector = [0,0,0,0]\n",
        "\n",
        "    # 0 - verbs, 1 - adjectives, 2 - nouns, 3 - adverbs\n",
        "    confusing_verb_count = 0\n",
        "    for word, tag in words_tagged:\n",
        "      if (tag.startswith('VB')):\n",
        "        word_vector[0] += len(wordnet.synsets(word, pos=wordnet.VERB))\n",
        "      elif (tag.startswith('JJ')):\n",
        "        word_vector[1] += len(wordnet.synsets(word, pos=wordnet.ADJ))\n",
        "      elif (tag.startswith('NN')):\n",
        "        word_vector[2] += len(wordnet.synsets(word, pos=wordnet.NOUN))\n",
        "      elif (tag.startswith('RB')):\n",
        "        word_vector[3] += len(wordnet.synsets(word, pos=wordnet.ADV))    \n",
        "    \n",
        "    return (word_vector[0] + word_vector[1] + word_vector[2] + word_vector[3])/len(words_tagged)\n",
        "  \n",
        "df['normalized_meanings'] = df['sentences'].apply(normalized_number_meanings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S3CAzWFLVnpU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Number of unique words in the text ##\n",
        "df[\"num_unique_words\"] = df[\"sentences\"].apply(lambda x: len(set(str(x).split())))\n",
        "\n",
        "## Number of stopwords in the text ##\n",
        "df[\"num_stop_words\"] = df[\"sentences\"].apply(lambda x: len([w for w in str(x).lower().split() if w in ENGLISH_STOP_WORDS]))\n",
        "\n",
        "## Number of punctuations in the text ##\n",
        "# df[\"num_punctuations\"] = df['sentences'].apply(lambda x: len([c for c in str(x) if c in str.punctuation]))\n",
        "\n",
        "## Number of title case words in the text ##\n",
        "df[\"num_words_upper\"] = df[\"sentences\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "\n",
        "## Number of title case words in the text ##\n",
        "df[\"num_words_title\"] = df[\"sentences\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
        "\n",
        "\n",
        "## Average length of the words in the text ##\n",
        "df[\"mean_word_len\"] = df[\"sentences\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UVGeKXJodQQt",
        "colab_type": "code",
        "outputId": "8f0d83f0-461e-419a-98c0-5d6da1af187e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "cell_type": "code",
      "source": [
        "# w2v = KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/datathon/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "# def avg_google_news_vec(sentence):\n",
        "#   doc = [word for word in sentence if word in w2v.vocab]\n",
        "#   return np.mean(w2v[doc], axis=0)\n",
        "\n",
        "# df['avg_google_news_vec'] = df['sentences'].apply(avg_google_news_vec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-13566b9c11f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/datathon/GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mavg_google_news_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode should be a string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shortcut_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/datathon/GoogleNews-vectors-negative300.bin'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "lyVx2BvoeuKX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/My Drive/datathon/loaded_language_phrases.txt') as f_loaded_lang:\n",
        "  loaded_lang=[loaded_word.rstrip() for loaded_word in f_loaded_lang.readlines()]\n",
        "\n",
        "def loaded_language_count(sent):\n",
        "  llc = 0\n",
        "  sent = sent.lower()\n",
        "  for loaded_word in loaded_lang:\n",
        "    if loaded_word in sent:\n",
        "      llc += 1\n",
        "  return llc\n",
        "      \n",
        "df['llc'] = df['sentences'].apply(loaded_language_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "inDgL5AeJlCB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/My Drive/datathon/exclamation_words.txt') as f_exlamation_words:\n",
        "  excl_words=[excl_word.rstrip() for excl_word in f_exlamation_words.readlines()]\n",
        "\n",
        "def excl_words_count(sent):\n",
        "  ewc = 0\n",
        "  sent = sent.lower().split()\n",
        "  for w in sent:\n",
        "    if w in excl_words:\n",
        "      ewc += 1\n",
        "  return ewc\n",
        "      \n",
        "df['ewc'] = df['sentences'].apply(excl_words_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYXygVJlygTN",
        "colab_type": "code",
        "outputId": "bf21f808-7835-4304-e63c-c187dfb8290d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14263, 58)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "metadata": {
        "id": "EPvKYPrmK-ef",
        "colab_type": "code",
        "outputId": "466fd7bd-e149-4105-d908-9ca57b8d0f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "cell_type": "code",
      "source": [
        "df.head(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>article</th>\n",
              "      <th>N_sentence</th>\n",
              "      <th>is_propaganda</th>\n",
              "      <th>sadness</th>\n",
              "      <th>joy</th>\n",
              "      <th>fear</th>\n",
              "      <th>disgust</th>\n",
              "      <th>anger</th>\n",
              "      <th>num_words</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>num_adjs</th>\n",
              "      <th>num_adv</th>\n",
              "      <th>sing_pro</th>\n",
              "      <th>pl_pro</th>\n",
              "      <th>is_question</th>\n",
              "      <th>is_exclamation</th>\n",
              "      <th>is_period</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>flesch_reading_ease</th>\n",
              "      <th>smog_index</th>\n",
              "      <th>flesch_kincaid_grade</th>\n",
              "      <th>coleman_liau_index</th>\n",
              "      <th>automated_readability_index</th>\n",
              "      <th>dale_chall_readability_score</th>\n",
              "      <th>difficult_words</th>\n",
              "      <th>linsear_write_formula</th>\n",
              "      <th>gunning_fog</th>\n",
              "      <th>text_standard</th>\n",
              "      <th>digits</th>\n",
              "      <th>punctuation_count</th>\n",
              "      <th>comma_count</th>\n",
              "      <th>semicolon_count</th>\n",
              "      <th>colon_count</th>\n",
              "      <th>spaces_count</th>\n",
              "      <th>apostrophes_count</th>\n",
              "      <th>amp_count</th>\n",
              "      <th>parenthesis_count</th>\n",
              "      <th>paragraph_count</th>\n",
              "      <th>pronouns</th>\n",
              "      <th>prepositions</th>\n",
              "      <th>coordinating_conjunctions</th>\n",
              "      <th>adjectives</th>\n",
              "      <th>adverbs</th>\n",
              "      <th>determiners</th>\n",
              "      <th>interjections</th>\n",
              "      <th>modals</th>\n",
              "      <th>nouns</th>\n",
              "      <th>personal_pronouns</th>\n",
              "      <th>verbs</th>\n",
              "      <th>word_len_gte_six</th>\n",
              "      <th>word_len_two_and_three</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>all_caps</th>\n",
              "      <th>capitalized</th>\n",
              "      <th>quotes_count</th>\n",
              "      <th>sentiment_score</th>\n",
              "      <th>normalized_meanings</th>\n",
              "      <th>num_unique_words</th>\n",
              "      <th>num_stop_words</th>\n",
              "      <th>num_words_upper</th>\n",
              "      <th>num_words_title</th>\n",
              "      <th>mean_word_len</th>\n",
              "      <th>llc</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>New Audio From The Night Of The Las Vegas Massacre Reveals That There Was “Another Active Shooter” In The Bar At The Top Of The Mandalay Bay Hotel During The Attack</td>\n",
              "      <td>704856340</td>\n",
              "      <td>1</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.093891</td>\n",
              "      <td>0.481352</td>\n",
              "      <td>0.238193</td>\n",
              "      <td>0.133129</td>\n",
              "      <td>0.162108</td>\n",
              "      <td>0.721268</td>\n",
              "      <td>0.460634</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.167677</td>\n",
              "      <td>0.518182</td>\n",
              "      <td>56.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>8.37</td>\n",
              "      <td>14.4</td>\n",
              "      <td>8.74</td>\n",
              "      <td>7</td>\n",
              "      <td>16.5</td>\n",
              "      <td>21.43</td>\n",
              "      <td>8th and 9th grade</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.182927</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.042683</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.042683</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.103659</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006098</td>\n",
              "      <td>0.04878</td>\n",
              "      <td>0.097561</td>\n",
              "      <td>0.817073</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.189024</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.015672</td>\n",
              "      <td>2.242424</td>\n",
              "      <td>25</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>4.322581</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                              sentences  \\\n",
              "0  New Audio From The Night Of The Las Vegas Massacre Reveals That There Was “Another Active Shooter” In The Bar At The Top Of The Mandalay Bay Hotel During The Attack   \n",
              "\n",
              "     article  N_sentence   is_propaganda   sadness       joy      fear  \\\n",
              "0  704856340           1  non-propaganda  0.093891  0.481352  0.238193   \n",
              "\n",
              "    disgust     anger  num_words  num_chars  num_adjs  num_adv  sing_pro  \\\n",
              "0  0.133129  0.162108   0.721268   0.460634         3        0         0   \n",
              "\n",
              "   pl_pro  is_question  is_exclamation  is_period  polarity  subjectivity  \\\n",
              "0       0            0               0          0  0.167677      0.518182   \n",
              "\n",
              "   flesch_reading_ease  smog_index  flesch_kincaid_grade  coleman_liau_index  \\\n",
              "0                56.93         0.0                  13.0                8.37   \n",
              "\n",
              "   automated_readability_index  dale_chall_readability_score  difficult_words  \\\n",
              "0                         14.4                          8.74                7   \n",
              "\n",
              "   linsear_write_formula  gunning_fog      text_standard  digits  \\\n",
              "0                   16.5        21.43  8th and 9th grade     0.0   \n",
              "\n",
              "   punctuation_count  comma_count  semicolon_count  colon_count  spaces_count  \\\n",
              "0                0.0          0.0              0.0          0.0      0.182927   \n",
              "\n",
              "   apostrophes_count  amp_count  parenthesis_count  paragraph_count  pronouns  \\\n",
              "0                0.0        0.0                0.0              0.0       0.0   \n",
              "\n",
              "   prepositions  coordinating_conjunctions  adjectives  adverbs  determiners  \\\n",
              "0      0.042683                        0.0         0.0      0.0     0.042683   \n",
              "\n",
              "   interjections  modals     nouns  personal_pronouns     verbs  \\\n",
              "0            0.0     0.0  0.103659                0.0  0.006098   \n",
              "\n",
              "   word_len_gte_six  word_len_two_and_three  avg_word_length  all_caps  \\\n",
              "0           0.04878                0.097561         0.817073       0.0   \n",
              "\n",
              "   capitalized  quotes_count  sentiment_score  normalized_meanings  \\\n",
              "0     0.189024           0.0        -0.015672             2.242424   \n",
              "\n",
              "   num_unique_words  num_stop_words  num_words_upper  num_words_title  \\\n",
              "0                25              16                0               31   \n",
              "\n",
              "   mean_word_len  llc  target  \n",
              "0       4.322581    0       0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "597xVeatBN27",
        "colab_type": "code",
        "outputId": "a6bd8307-79be-443b-987d-7e078469d286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentences', 'article', 'N_sentence', 'is_propaganda', 'sadness', 'joy',\n",
              "       'fear', 'disgust', 'anger', 'num_words', 'num_chars', 'num_adjs',\n",
              "       'num_adv', 'sing_pro', 'pl_pro', 'is_question', 'is_exclamation',\n",
              "       'is_period', 'polarity', 'subjectivity', 'flesch_reading_ease',\n",
              "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
              "       'automated_readability_index', 'dale_chall_readability_score',\n",
              "       'difficult_words', 'linsear_write_formula', 'gunning_fog', 'digits',\n",
              "       'punctuation_count', 'comma_count', 'semicolon_count', 'colon_count',\n",
              "       'spaces_count', 'apostrophes_count', 'amp_count', 'parenthesis_count',\n",
              "       'paragraph_count', 'pronouns', 'prepositions',\n",
              "       'coordinating_conjunctions', 'adjectives', 'adverbs', 'determiners',\n",
              "       'interjections', 'modals', 'nouns', 'personal_pronouns', 'verbs',\n",
              "       'word_len_gte_six', 'word_len_two_and_three', 'avg_word_length',\n",
              "       'all_caps', 'capitalized', 'quotes_count', 'sentiment_score',\n",
              "       'normalized_meanings', 'num_unique_words', 'num_stop_words',\n",
              "       'num_words_upper', 'num_words_title', 'mean_word_len'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "3hrmJ2I48cmw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SEED = 666"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tzfvJK488uP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['target'] = df['is_propaganda'].map({'propaganda': 1, 'non-propaganda': 0})\n",
        "df.drop(['is_propaganda'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9A4xzB_U9Mz8",
        "colab_type": "code",
        "outputId": "cf64bc76-80ea-4d40-bb55-f1d4fdef02a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "y = df[['target']]\n",
        "\n",
        "X_train, X_rest, y_train, y_rest = train_test_split(\n",
        "        df, y,stratify=y, test_size=0.2, random_state=SEED)\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_rest, y_rest,stratify=y_rest, test_size=0.5, random_state=SEED)\n",
        "\n",
        "print(X_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11410, 65)\n",
            "(1426, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9rGyNDpbKHvt",
        "colab_type": "code",
        "outputId": "b8ada3a3-0664-4bc7-c8c4-ce00ccbf30a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        }
      },
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train[['num_words', 'num_chars']] = scaler.fit_transform(X_train[['num_words', 'num_chars']])\n",
        "X_val[['num_words', 'num_chars']] = scaler.transform(X_val[['num_words', 'num_chars']])\n",
        "X_test[['num_words', 'num_chars']] = scaler.transform(X_test[['num_words', 'num_chars']])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, **fit_params).transform(X)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self.obj[item] = s\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self.obj[item] = s\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self.obj[item] = s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1WseDulGv1q_",
        "colab_type": "code",
        "outputId": "599cc512-e06d-4824-d095-b6ec8f1f9c42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>article</th>\n",
              "      <th>N_sentence</th>\n",
              "      <th>is_propaganda</th>\n",
              "      <th>sadness</th>\n",
              "      <th>joy</th>\n",
              "      <th>fear</th>\n",
              "      <th>disgust</th>\n",
              "      <th>anger</th>\n",
              "      <th>num_words</th>\n",
              "      <th>...</th>\n",
              "      <th>quotes_count</th>\n",
              "      <th>sentiment_score</th>\n",
              "      <th>normalized_meanings</th>\n",
              "      <th>num_unique_words</th>\n",
              "      <th>num_stop_words</th>\n",
              "      <th>num_words_upper</th>\n",
              "      <th>num_words_title</th>\n",
              "      <th>mean_word_len</th>\n",
              "      <th>llc</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>The Nuncio added that Littleton had already fo...</td>\n",
              "      <td>782086447</td>\n",
              "      <td>32</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.299419</td>\n",
              "      <td>0.027867</td>\n",
              "      <td>0.243245</td>\n",
              "      <td>0.177422</td>\n",
              "      <td>0.064517</td>\n",
              "      <td>41</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.022667</td>\n",
              "      <td>3.282609</td>\n",
              "      <td>37</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5.414634</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>FACT asked the Senate ethics committee to prob...</td>\n",
              "      <td>999000147</td>\n",
              "      <td>22</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.389350</td>\n",
              "      <td>0.018483</td>\n",
              "      <td>0.065846</td>\n",
              "      <td>0.362440</td>\n",
              "      <td>0.294696</td>\n",
              "      <td>21</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.030220</td>\n",
              "      <td>2.120000</td>\n",
              "      <td>20</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>6.047619</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>“There is a silence among many who call themse...</td>\n",
              "      <td>763260610</td>\n",
              "      <td>9</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.688505</td>\n",
              "      <td>0.061684</td>\n",
              "      <td>0.211507</td>\n",
              "      <td>0.057255</td>\n",
              "      <td>0.112880</td>\n",
              "      <td>10</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.008757</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.700000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>The President of AMANA, Sofian Zakkout, has re...</td>\n",
              "      <td>728169864</td>\n",
              "      <td>43</td>\n",
              "      <td>propaganda</td>\n",
              "      <td>0.063294</td>\n",
              "      <td>0.450346</td>\n",
              "      <td>0.035087</td>\n",
              "      <td>0.310004</td>\n",
              "      <td>0.138506</td>\n",
              "      <td>18</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.099388</td>\n",
              "      <td>2.291667</td>\n",
              "      <td>17</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4.500000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>Date of erection: 1994</td>\n",
              "      <td>761334950</td>\n",
              "      <td>82</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.044200</td>\n",
              "      <td>0.195751</td>\n",
              "      <td>0.011521</td>\n",
              "      <td>0.159430</td>\n",
              "      <td>0.030789</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.015625</td>\n",
              "      <td>2.200000</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.750000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 65 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentences    article  N_sentence  \\\n",
              "31  The Nuncio added that Littleton had already fo...  782086447          32   \n",
              "21  FACT asked the Senate ethics committee to prob...  999000147          22   \n",
              "8   “There is a silence among many who call themse...  763260610           9   \n",
              "42  The President of AMANA, Sofian Zakkout, has re...  728169864          43   \n",
              "81                             Date of erection: 1994  761334950          82   \n",
              "\n",
              "     is_propaganda   sadness       joy      fear   disgust     anger  \\\n",
              "31  non-propaganda  0.299419  0.027867  0.243245  0.177422  0.064517   \n",
              "21  non-propaganda  0.389350  0.018483  0.065846  0.362440  0.294696   \n",
              "8   non-propaganda  0.688505  0.061684  0.211507  0.057255  0.112880   \n",
              "42      propaganda  0.063294  0.450346  0.035087  0.310004  0.138506   \n",
              "81  non-propaganda  0.044200  0.195751  0.011521  0.159430  0.030789   \n",
              "\n",
              "    num_words   ...    quotes_count  sentiment_score  normalized_meanings  \\\n",
              "31         41   ...             0.0         0.022667             3.282609   \n",
              "21         21   ...             0.0         0.030220             2.120000   \n",
              "8          10   ...             0.0        -0.008757             5.000000   \n",
              "42         18   ...             0.0         0.099388             2.291667   \n",
              "81          4   ...             0.0        -0.015625             2.200000   \n",
              "\n",
              "    num_unique_words  num_stop_words  num_words_upper  num_words_title  \\\n",
              "31                37              22                0                5   \n",
              "21                20               4                1                9   \n",
              "8                 10               7                0                1   \n",
              "42                17               7                1                7   \n",
              "81                 4               1                0                1   \n",
              "\n",
              "    mean_word_len  llc  target  \n",
              "31       5.414634    0       0  \n",
              "21       6.047619    0       0  \n",
              "8        4.700000    0       0  \n",
              "42       4.500000    0       1  \n",
              "81       4.750000    0       0  \n",
              "\n",
              "[5 rows x 65 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "lcwoeody9YOc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Maybe try different params/vectorisers, stemming etc.? - got these from the winning system for task 1.\n",
        "vectorizer = TfidfVectorizer(min_df = 3, max_df=0.5,  max_features=None, \n",
        "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
        "            stop_words = 'english')\n",
        "\n",
        "x_train = vectorizer.fit_transform(X_train['sentences'])\n",
        "x_val = vectorizer.transform(X_val['sentences'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "009aOutCiC4W",
        "colab_type": "code",
        "outputId": "3286706e-ace8-4237-9a32-f273b9e77b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "pipe_lrSVC = Pipeline([('scaler', StandardScaler()),('clf', LinearSVC(C=1.0, class_weight='balanced', multi_class='ovr', random_state=SEED))])\n",
        "pipe_lrSVC.fit(X_train[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'num_words', 'num_chars', 'num_adjs',\n",
        "       'num_adv', 'sing_pro', 'pl_pro', 'is_question', 'is_exclamation',\n",
        "       'is_period', 'polarity', 'subjectivity', 'flesch_reading_ease',\n",
        "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
        "       'automated_readability_index', 'dale_chall_readability_score',\n",
        "       'difficult_words', 'linsear_write_formula', 'gunning_fog', 'digits',\n",
        "       'punctuation_count', 'comma_count', 'semicolon_count', 'colon_count',\n",
        "       'spaces_count', 'apostrophes_count', 'amp_count', 'parenthesis_count',\n",
        "       'paragraph_count', 'pronouns', 'prepositions',\n",
        "       'coordinating_conjunctions', 'adjectives', 'adverbs', 'determiners',\n",
        "       'interjections', 'modals', 'nouns', 'personal_pronouns', 'verbs',\n",
        "       'word_len_gte_six', 'word_len_two_and_three', 'avg_word_length',\n",
        "       'all_caps', 'capitalized', 'quotes_count', 'sentiment_score',\n",
        "       'normalized_meanings', 'num_unique_words', 'num_stop_words',\n",
        "       'num_words_upper', 'num_words_title', 'mean_word_len']], y_train)\n",
        "y_pred = pipe_lrSVC.predict(X_val[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'num_words', 'num_chars', 'num_adjs',\n",
        "       'num_adv', 'sing_pro', 'pl_pro', 'is_question', 'is_exclamation',\n",
        "       'is_period', 'polarity', 'subjectivity', 'flesch_reading_ease',\n",
        "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
        "       'automated_readability_index', 'dale_chall_readability_score',\n",
        "       'difficult_words', 'linsear_write_formula', 'gunning_fog', 'digits',\n",
        "       'punctuation_count', 'comma_count', 'semicolon_count', 'colon_count',\n",
        "       'spaces_count', 'apostrophes_count', 'amp_count', 'parenthesis_count',\n",
        "       'paragraph_count', 'pronouns', 'prepositions',\n",
        "       'coordinating_conjunctions', 'adjectives', 'adverbs', 'determiners',\n",
        "       'interjections', 'modals', 'nouns', 'personal_pronouns', 'verbs',\n",
        "       'word_len_gte_six', 'word_len_two_and_three', 'avg_word_length',\n",
        "       'all_caps', 'capitalized', 'quotes_count', 'sentiment_score',\n",
        "       'normalized_meanings', 'num_unique_words', 'num_stop_words',\n",
        "       'num_words_upper', 'num_words_title', 'mean_word_len']])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "uEturQmWQfPw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4urpt0aEOomC",
        "colab_type": "code",
        "outputId": "f3a46870-01b3-41d3-ce28-5bd910a9e610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# NN without scaling\n",
        "\n",
        "print(f1_score(y_val.values.ravel(), y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3009523809523809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zY_Fp1INO1-7",
        "colab_type": "code",
        "outputId": "89f134c6-2cc9-43c9-a1f2-e07f69d23cd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# NN with scaling\n",
        "\n",
        "print(f1_score(y_val.values.ravel(), y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.37623762376237624\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xTBECTmoN02H",
        "colab_type": "code",
        "outputId": "ebb7f8a1-e66b-4d20-8cd8-a8da7424eba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# SVM without scaling\n",
        "\n",
        "print(f1_score(y_val.values.ravel(), y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4428002276607854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZYGDkonhOI_6",
        "colab_type": "code",
        "outputId": "73de353f-21f7-4c3b-ff2b-5544cf7296ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# SVM with scaling\n",
        "\n",
        "print(f1_score(y_val.values.ravel(), y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5004926108374385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aDFouQtE0LEm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# NN without tf idf\n",
        "\n",
        "print(classification_report(y_val.values.ravel(), y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AUAop2u0vtgF",
        "colab_type": "code",
        "outputId": "9b17ecf5-89a7-4f37-d99f-04ea026954ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "# NN without tf idf\n",
        "\n",
        "print(classification_report(y_val.values.ravel(), y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.83      0.79      1032\n",
            "           1       0.42      0.34      0.38       394\n",
            "\n",
            "   micro avg       0.69      0.69      0.69      1426\n",
            "   macro avg       0.60      0.58      0.59      1426\n",
            "weighted avg       0.67      0.69      0.68      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uTkYXRkgwAzv",
        "colab_type": "code",
        "outputId": "6c1986d4-88a1-4025-914d-276e34dfac91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "# NN without tf idf no scale\n",
        "\n",
        "print(classification_report(y_val.values.ravel(), y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.95      0.84      1032\n",
            "           1       0.60      0.20      0.30       394\n",
            "\n",
            "   micro avg       0.74      0.74      0.74      1426\n",
            "   macro avg       0.68      0.58      0.57      1426\n",
            "weighted avg       0.71      0.74      0.69      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xRJuw8-po4Rk",
        "colab_type": "code",
        "outputId": "2518b4f2-ab56-471d-bb1f-d5b886dddebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "# SVM except tf idf\n",
        "print(classification_report(y_val.values.ravel(), y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.65      0.72      1032\n",
            "           1       0.41      0.64      0.50       394\n",
            "\n",
            "   micro avg       0.64      0.64      0.64      1426\n",
            "   macro avg       0.62      0.64      0.61      1426\n",
            "weighted avg       0.71      0.64      0.66      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iH7hBRXsFONJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# All feats\n",
        "#hstack([x_train, X_train[['sadness', 'joy',\n",
        "#        'fear', 'disgust', 'anger', 'num_words', 'num_chars', 'num_adjs',\n",
        "#        'num_adv', 'sing_pro', 'pl_pro', 'is_question', 'is_exclamation',\n",
        "#        'is_period', 'polarity', 'subjectivity', 'flesch_reading_ease',\n",
        "#        'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
        "#        'automated_readability_index', 'dale_chall_readability_score',\n",
        "#        'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
        "#        'digits', 'punctuation_count', 'comma_count',\n",
        "#        'semicolon_count', 'colon_count', 'spaces_count', 'apostrophes_count',\n",
        "#        'amp_count', 'parenthesis_count', 'paragraph_count', 'pronouns',\n",
        "#        'prepositions', 'coordinating_conjunctions', 'adjectives', 'adverbs',\n",
        "#        'determiners', 'interjections', 'modals', 'nouns', 'personal_pronouns',\n",
        "#        'verbs', 'word_len_gte_six', 'word_len_two_and_three',\n",
        "#        'avg_word_length', 'all_caps', 'capitalized', 'quotes_count',\n",
        "#        'sentiment_score', 'normalized_meanings', 'num_unique_words',\n",
        "#        'num_stop_words', 'num_words_upper', 'num_words_title', 'mean_word_len',\n",
        "#        'llc']].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sWjr7UzuBAip",
        "colab_type": "code",
        "outputId": "921404b6-d8b2-40d2-e899-24de00c18699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "model = LinearSVC(C=1.0, class_weight='balanced', multi_class='ovr', random_state=SEED)\n",
        "# Train only with text data\n",
        "model.fit(x_train, y_train.values.ravel())\n",
        "\n",
        "\n",
        "# Or with more features\n",
        "# model.fit(hstack([x_train, X_train[['sadness', 'joy', 'fear', 'disgust' , 'anger']].values]), y_train.values.ravel())\n",
        "\n",
        "# model.fit(hstack([x_train, X_train[['sadness', 'joy',\n",
        "#        'fear', 'disgust', 'anger', 'polarity', 'subjectivity', 'llc', 'is_question', 'is_exclamation',\n",
        "#        'is_period', 'flesch_reading_ease',\n",
        "#        'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
        "#        'automated_readability_index', 'dale_chall_readability_score',\n",
        "#        'difficult_words', 'linsear_write_formula', 'gunning_fog']].values]), y_train.values.ravel())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n",
              "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "     multi_class='ovr', penalty='l2', random_state=666, tol=0.0001,\n",
              "     verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "IPN1j1d8Dzi5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pd.set_option('max_colwidth', 800)\n",
        "pd.set_option('display.max_columns', 500)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V2QHYIB_SppZ",
        "colab_type": "code",
        "outputId": "1b667e8a-c7e2-4b23-afef-8960175aa87a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Only tf-idf\n",
        "\n",
        "y_val_predict = model.predict(x_val)\n",
        "print(f1_score(y_val.values.ravel(), y_val_predict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.506508875739645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "whV17XC0Bu1h",
        "colab_type": "code",
        "outputId": "51e6d329-26a0-4b7d-8101-ac1a28e9b9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "# Only tf-idf\n",
        "\n",
        "y_val_predict = model.predict(x_val)\n",
        "print(classification_report(y_val.values.ravel(), y_val_predict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.77      0.79      1032\n",
            "           1       0.47      0.54      0.51       394\n",
            "\n",
            "   micro avg       0.71      0.71      0.71      1426\n",
            "   macro avg       0.64      0.66      0.65      1426\n",
            "weighted avg       0.72      0.71      0.71      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E2dmY9Wyek6E",
        "colab_type": "code",
        "outputId": "1c5ca961-5c96-4892-90a6-081fb7417b93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "# All features\n",
        "y_val_predict = model.predict(hstack([x_val, X_val[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'num_words', 'num_chars', 'num_adjs',\n",
        "       'num_adv', 'sing_pro', 'pl_pro', 'is_question', 'is_exclamation',\n",
        "       'is_period', 'polarity', 'subjectivity', 'flesch_reading_ease',\n",
        "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
        "       'automated_readability_index', 'dale_chall_readability_score',\n",
        "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
        "       'digits', 'punctuation_count', 'comma_count',\n",
        "       'semicolon_count', 'colon_count', 'spaces_count', 'apostrophes_count',\n",
        "       'amp_count', 'parenthesis_count', 'paragraph_count', 'pronouns',\n",
        "       'prepositions', 'coordinating_conjunctions', 'adjectives', 'adverbs',\n",
        "       'determiners', 'interjections', 'modals', 'nouns', 'personal_pronouns',\n",
        "       'verbs', 'word_len_gte_six', 'word_len_two_and_three',\n",
        "       'avg_word_length', 'all_caps', 'capitalized', 'quotes_count',\n",
        "       'sentiment_score', 'normalized_meanings', 'num_unique_words',\n",
        "       'num_stop_words', 'num_words_upper', 'num_words_title', 'mean_word_len',\n",
        "       'llc']].values]))\n",
        "\n",
        "print(classification_report(y_val.values.ravel(), y_val_predict))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.66      0.73      1032\n",
            "           1       0.42      0.63      0.50       394\n",
            "\n",
            "   micro avg       0.65      0.65      0.65      1426\n",
            "   macro avg       0.62      0.65      0.62      1426\n",
            "weighted avg       0.71      0.65      0.67      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9bjFyS2XFaAd",
        "colab_type": "code",
        "outputId": "1d8a7b1d-4450-46fe-9d60-8729c63779b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "# Tf-idf + emotions\n",
        "\n",
        "y_val_predict = model.predict(hstack([x_val, X_val[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger']].values]))\n",
        "\n",
        "print(classification_report(y_val.values.ravel(), y_val_predict))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.77      0.79      1032\n",
            "           1       0.48      0.55      0.51       394\n",
            "\n",
            "   micro avg       0.71      0.71      0.71      1426\n",
            "   macro avg       0.65      0.66      0.65      1426\n",
            "weighted avg       0.72      0.71      0.72      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gEAihJ5CEvot",
        "colab_type": "code",
        "outputId": "4404cc18-994f-4ae2-e92c-edcd51527dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "# Tf-idf + emotions + polarity\n",
        "\n",
        "y_val_predict = model.predict(hstack([x_val, X_val[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'polarity']].values]))\n",
        "\n",
        "print(classification_report(y_val.values.ravel(), y_val_predict))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.77      0.79      1032\n",
            "           1       0.48      0.55      0.51       394\n",
            "\n",
            "   micro avg       0.71      0.71      0.71      1426\n",
            "   macro avg       0.65      0.66      0.65      1426\n",
            "weighted avg       0.72      0.71      0.72      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Eq3yfGl3F-9v",
        "colab_type": "code",
        "outputId": "93f23af8-c1ec-41e9-b19e-dfd4323e7674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "# Tf-idf + emotions + polarity + subjectivity\n",
        "\n",
        "y_val_predict = model.predict(hstack([x_val, X_val[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'polarity', 'subjectivity']].values]))\n",
        "\n",
        "print(classification_report(y_val.values.ravel(), y_val_predict))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.78      0.80      1032\n",
            "           1       0.49      0.55      0.52       394\n",
            "\n",
            "   micro avg       0.71      0.71      0.71      1426\n",
            "   macro avg       0.65      0.66      0.66      1426\n",
            "weighted avg       0.73      0.71      0.72      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RUdHyE2QGLN2",
        "colab_type": "code",
        "outputId": "63eff34f-1256-4561-c62f-d45206523d00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "# Tf-idf + emotions + polarity + subjectivity + loaded language\n",
        "\n",
        "y_val_predict = model.predict(hstack([x_val, X_val[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'polarity', 'subjectivity', 'llc']].values]))\n",
        "\n",
        "print(classification_report(y_val.values.ravel(), y_val_predict))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.78      0.80      1032\n",
            "           1       0.49      0.57      0.53       394\n",
            "\n",
            "   micro avg       0.72      0.72      0.72      1426\n",
            "   macro avg       0.66      0.67      0.66      1426\n",
            "weighted avg       0.73      0.72      0.72      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hAsyJdlSGcTL",
        "colab_type": "code",
        "outputId": "eb105ab2-3e65-4598-839e-1153208c8859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "# Tf-idf + emotions + polarity + subjectivity + loaded language\n",
        "\n",
        "y_val_predict = model.predict(hstack([x_val, X_val[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'polarity', 'subjectivity', 'llc', 'num_words', 'num_chars']].values]))\n",
        "\n",
        "print(classification_report(y_val.values.ravel(), y_val_predict))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.79      0.80      1032\n",
            "           1       0.50      0.55      0.52       394\n",
            "\n",
            "   micro avg       0.72      0.72      0.72      1426\n",
            "   macro avg       0.66      0.67      0.66      1426\n",
            "weighted avg       0.73      0.72      0.73      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uYZA9_3UG-35",
        "colab_type": "code",
        "outputId": "f15caf7f-638a-420b-db00-632ed08c2c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "# Tf-idf + emotions + polarity + subjectivity + loaded language\n",
        "\n",
        "y_val_predict = model.predict(hstack([x_val, X_val[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'polarity', 'subjectivity', 'llc', 'is_question', 'is_exclamation',\n",
        "       'is_period']].values]))\n",
        "\n",
        "print(classification_report(y_val.values.ravel(), y_val_predict))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.79      0.80      1032\n",
            "           1       0.50      0.56      0.53       394\n",
            "\n",
            "   micro avg       0.72      0.72      0.72      1426\n",
            "   macro avg       0.66      0.67      0.67      1426\n",
            "weighted avg       0.74      0.72      0.73      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VvrAP55OLU1g",
        "colab_type": "code",
        "outputId": "ef812fb2-588b-44fc-ee15-d53131ad828c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "# Tf-idf + emotions + polarity + subjectivity + loaded language\n",
        "\n",
        "y_val_predict = model.predict(hstack([x_val, X_val[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'polarity', 'subjectivity', 'llc', 'is_question', 'is_exclamation',\n",
        "       'is_period', 'normalized_meanings']].values]))\n",
        "\n",
        "print(classification_report(y_val.values.ravel(), y_val_predict))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.78      0.80      1032\n",
            "           1       0.49      0.56      0.52       394\n",
            "\n",
            "   micro avg       0.72      0.72      0.72      1426\n",
            "   macro avg       0.65      0.67      0.66      1426\n",
            "weighted avg       0.73      0.72      0.72      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WSBXUmVjLpWr",
        "colab_type": "code",
        "outputId": "6c28b334-32eb-46a1-ada5-aa9c2d7d7e23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Tf-idf + emotions + polarity + subjectivity + loaded language\n",
        "\n",
        "y_val_predict = model.predict(hstack([x_val, X_val[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'polarity', 'subjectivity', 'llc', 'is_question', 'is_exclamation',\n",
        "       'is_period', 'flesch_reading_ease',\n",
        "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
        "       'automated_readability_index', 'dale_chall_readability_score',\n",
        "       'difficult_words', 'linsear_write_formula', 'gunning_fog'\n",
        "      ]].values]))\n",
        "\n",
        "print(classification_report(y_val.values.ravel(), y_val_predict))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.90      0.83      1032\n",
            "           1       0.51      0.28      0.36       394\n",
            "\n",
            "   micro avg       0.73      0.73      0.73      1426\n",
            "   macro avg       0.64      0.59      0.59      1426\n",
            "weighted avg       0.70      0.73      0.70      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kfjHSJz1MMLY",
        "colab_type": "code",
        "outputId": "415a7ba3-a649-4ce5-8069-a8de935086cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Tf-idf + emotions + polarity + subjectivity + loaded language\n",
        "\n",
        "y_val_predict = model.predict(hstack([x_val, X_val[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'polarity', 'subjectivity', 'llc', 'is_question', 'is_exclamation',\n",
        "       'is_period', 'flesch_reading_ease',\n",
        "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
        "       'automated_readability_index', 'dale_chall_readability_score',\n",
        "       'difficult_words', 'linsear_write_formula', 'gunning_fog'\n",
        "      ]].values]))\n",
        "\n",
        "print(classification_report(y_val.values.ravel(), y_val_predict))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.62      0.71      1032\n",
            "           1       0.42      0.71      0.53       394\n",
            "\n",
            "   micro avg       0.64      0.64      0.64      1426\n",
            "   macro avg       0.63      0.67      0.62      1426\n",
            "weighted avg       0.73      0.64      0.66      1426\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rweILIQpMewX",
        "colab_type": "code",
        "outputId": "aa06610f-b668-44a3-89fb-d19228ec0f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2383
        }
      },
      "cell_type": "code",
      "source": [
        "for blah in ['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'num_words', 'num_chars', 'num_adjs',\n",
        "       'num_adv', 'sing_pro', 'pl_pro', 'is_question', 'is_exclamation',\n",
        "       'is_period', 'polarity', 'subjectivity', 'flesch_reading_ease',\n",
        "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
        "       'automated_readability_index', 'dale_chall_readability_score',\n",
        "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
        "       'digits', 'punctuation_count', 'comma_count',\n",
        "       'semicolon_count', 'colon_count', 'spaces_count', 'apostrophes_count',\n",
        "       'amp_count', 'parenthesis_count', 'paragraph_count', 'pronouns',\n",
        "       'prepositions', 'coordinating_conjunctions', 'adjectives', 'adverbs',\n",
        "       'determiners', 'interjections', 'modals', 'nouns', 'personal_pronouns',\n",
        "       'verbs', 'word_len_gte_six', 'word_len_two_and_three',\n",
        "       'avg_word_length', 'all_caps', 'capitalized', 'quotes_count',\n",
        "       'sentiment_score', 'normalized_meanings', 'num_unique_words',\n",
        "       'num_stop_words', 'num_words_upper', 'num_words_title', 'mean_word_len',\n",
        "       'llc']:\n",
        "\n",
        "  model = LinearSVC(C=1.0, class_weight='balanced', multi_class='ovr', random_state=SEED)\n",
        "  # Train only with text data\n",
        "  print(blah)\n",
        "  model.fit(x_train, y_train.values.ravel())\n",
        "\n",
        "  \n",
        "  # Or with more features\n",
        "  # model.fit(hstack([x_train, X_train[['sadness', 'joy', 'fear', 'disgust' , 'anger']].values]), y_train.values.ravel())\n",
        "\n",
        "  model.fit(X_train[[blah]].values, y_train.values.ravel())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sadness\n",
            "joy\n",
            "fear\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "disgust\n",
            "anger\n",
            "num_words\n",
            "num_chars\n",
            "num_adjs\n",
            "num_adv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "sing_pro\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pl_pro\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "is_question\n",
            "is_exclamation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "is_period\n",
            "polarity\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "subjectivity\n",
            "flesch_reading_ease\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "smog_index\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "flesch_kincaid_grade\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "coleman_liau_index\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "automated_readability_index\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dale_chall_readability_score\n",
            "difficult_words\n",
            "linsear_write_formula\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "gunning_fog\n",
            "digits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "punctuation_count\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "comma_count\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "semicolon_count\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "colon_count\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "spaces_count\n",
            "apostrophes_count\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "amp_count\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "parenthesis_count\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "paragraph_count\n",
            "pronouns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "prepositions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "coordinating_conjunctions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "adjectives\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "adverbs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "determiners\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "interjections\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "modals\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "nouns\n",
            "personal_pronouns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "verbs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "word_len_gte_six\n",
            "word_len_two_and_three\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_word_length\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "all_caps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "capitalized\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "quotes_count\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "sentiment_score\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "normalized_meanings\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_unique_words\n",
            "num_stop_words\n",
            "num_words_upper\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_words_title\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mean_word_len\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "llc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-FwRLLPjVcTy",
        "colab_type": "code",
        "outputId": "343f899e-d7e1-439e-b581-3865a2b789d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1701
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>article</th>\n",
              "      <th>N_sentence</th>\n",
              "      <th>is_propaganda</th>\n",
              "      <th>sadness</th>\n",
              "      <th>joy</th>\n",
              "      <th>fear</th>\n",
              "      <th>disgust</th>\n",
              "      <th>anger</th>\n",
              "      <th>num_words</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>num_adjs</th>\n",
              "      <th>num_adv</th>\n",
              "      <th>sing_pro</th>\n",
              "      <th>pl_pro</th>\n",
              "      <th>is_question</th>\n",
              "      <th>is_exclamation</th>\n",
              "      <th>is_period</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>flesch_reading_ease</th>\n",
              "      <th>smog_index</th>\n",
              "      <th>flesch_kincaid_grade</th>\n",
              "      <th>coleman_liau_index</th>\n",
              "      <th>automated_readability_index</th>\n",
              "      <th>dale_chall_readability_score</th>\n",
              "      <th>difficult_words</th>\n",
              "      <th>linsear_write_formula</th>\n",
              "      <th>gunning_fog</th>\n",
              "      <th>text_standard</th>\n",
              "      <th>digits</th>\n",
              "      <th>punctuation_count</th>\n",
              "      <th>comma_count</th>\n",
              "      <th>semicolon_count</th>\n",
              "      <th>colon_count</th>\n",
              "      <th>spaces_count</th>\n",
              "      <th>apostrophes_count</th>\n",
              "      <th>amp_count</th>\n",
              "      <th>parenthesis_count</th>\n",
              "      <th>paragraph_count</th>\n",
              "      <th>pronouns</th>\n",
              "      <th>prepositions</th>\n",
              "      <th>coordinating_conjunctions</th>\n",
              "      <th>adjectives</th>\n",
              "      <th>adverbs</th>\n",
              "      <th>determiners</th>\n",
              "      <th>interjections</th>\n",
              "      <th>modals</th>\n",
              "      <th>nouns</th>\n",
              "      <th>personal_pronouns</th>\n",
              "      <th>verbs</th>\n",
              "      <th>word_len_gte_six</th>\n",
              "      <th>word_len_two_and_three</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>all_caps</th>\n",
              "      <th>capitalized</th>\n",
              "      <th>quotes_count</th>\n",
              "      <th>sentiment_score</th>\n",
              "      <th>normalized_meanings</th>\n",
              "      <th>num_unique_words</th>\n",
              "      <th>num_stop_words</th>\n",
              "      <th>num_words_upper</th>\n",
              "      <th>num_words_title</th>\n",
              "      <th>mean_word_len</th>\n",
              "      <th>llc</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>The Nuncio added that Littleton had already forwarded his Memorandum to about twenty people, including civil and ecclesiastical judicial authorities, police and lawyers, in June 2006, and that it was therefore very likely that the news would soon be made public.</td>\n",
              "      <td>782086447</td>\n",
              "      <td>32</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.235276</td>\n",
              "      <td>-0.911798</td>\n",
              "      <td>0.848839</td>\n",
              "      <td>0.053167</td>\n",
              "      <td>-0.723438</td>\n",
              "      <td>1.405955</td>\n",
              "      <td>1.559952</td>\n",
              "      <td>1.903499</td>\n",
              "      <td>2.510659</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.25546</td>\n",
              "      <td>-0.118878</td>\n",
              "      <td>0.565002</td>\n",
              "      <td>-0.191990</td>\n",
              "      <td>0.754552</td>\n",
              "      <td>-1.132919</td>\n",
              "      <td>-0.040635</td>\n",
              "      <td>1.493139</td>\n",
              "      <td>0.652393</td>\n",
              "      <td>1.404653</td>\n",
              "      <td>0.381251</td>\n",
              "      <td>1.072258</td>\n",
              "      <td>1.577896</td>\n",
              "      <td>0.948402</td>\n",
              "      <td>20th and 21st grade</td>\n",
              "      <td>0.211000</td>\n",
              "      <td>-0.209878</td>\n",
              "      <td>0.746223</td>\n",
              "      <td>-0.093158</td>\n",
              "      <td>-0.173470</td>\n",
              "      <td>-0.030168</td>\n",
              "      <td>-0.177548</td>\n",
              "      <td>-0.048092</td>\n",
              "      <td>-0.095577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.267249</td>\n",
              "      <td>-0.012941</td>\n",
              "      <td>0.881524</td>\n",
              "      <td>0.686396</td>\n",
              "      <td>0.406458</td>\n",
              "      <td>-0.692975</td>\n",
              "      <td>-0.045957</td>\n",
              "      <td>0.227527</td>\n",
              "      <td>-0.589289</td>\n",
              "      <td>-0.324255</td>\n",
              "      <td>-0.223071</td>\n",
              "      <td>0.315972</td>\n",
              "      <td>-0.528557</td>\n",
              "      <td>0.007077</td>\n",
              "      <td>-0.252268</td>\n",
              "      <td>-0.493434</td>\n",
              "      <td>-0.205636</td>\n",
              "      <td>0.363518</td>\n",
              "      <td>0.199365</td>\n",
              "      <td>1.555694</td>\n",
              "      <td>1.675983</td>\n",
              "      <td>-0.444427</td>\n",
              "      <td>0.496643</td>\n",
              "      <td>0.297889</td>\n",
              "      <td>-0.414111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>FACT asked the Senate ethics committee to probe fundraising emails sent by Ms. Warren, Massachusetts Democrat, and Ms. Harris, California Democrat.</td>\n",
              "      <td>999000147</td>\n",
              "      <td>22</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>0.743346</td>\n",
              "      <td>-0.960339</td>\n",
              "      <td>-0.507128</td>\n",
              "      <td>1.215750</td>\n",
              "      <td>0.886222</td>\n",
              "      <td>0.026341</td>\n",
              "      <td>0.266129</td>\n",
              "      <td>-0.518542</td>\n",
              "      <td>-0.834346</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.25546</td>\n",
              "      <td>-0.118878</td>\n",
              "      <td>0.565002</td>\n",
              "      <td>-0.191990</td>\n",
              "      <td>-0.997068</td>\n",
              "      <td>-0.069015</td>\n",
              "      <td>17.823058</td>\n",
              "      <td>-0.426289</td>\n",
              "      <td>0.540970</td>\n",
              "      <td>-0.228516</td>\n",
              "      <td>0.528296</td>\n",
              "      <td>0.620352</td>\n",
              "      <td>-0.916639</td>\n",
              "      <td>-0.009407</td>\n",
              "      <td>9th and 10th grade</td>\n",
              "      <td>-0.208090</td>\n",
              "      <td>0.089794</td>\n",
              "      <td>1.248788</td>\n",
              "      <td>-0.093158</td>\n",
              "      <td>-0.173470</td>\n",
              "      <td>-0.544990</td>\n",
              "      <td>-0.177548</td>\n",
              "      <td>-0.048092</td>\n",
              "      <td>-0.095577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.754521</td>\n",
              "      <td>-0.918709</td>\n",
              "      <td>0.260994</td>\n",
              "      <td>-0.825508</td>\n",
              "      <td>-0.613536</td>\n",
              "      <td>-0.746294</td>\n",
              "      <td>-0.045957</td>\n",
              "      <td>-0.374450</td>\n",
              "      <td>0.592356</td>\n",
              "      <td>-0.597938</td>\n",
              "      <td>-0.196680</td>\n",
              "      <td>0.972127</td>\n",
              "      <td>-0.660229</td>\n",
              "      <td>0.513972</td>\n",
              "      <td>0.212546</td>\n",
              "      <td>0.761029</td>\n",
              "      <td>-0.205636</td>\n",
              "      <td>0.475084</td>\n",
              "      <td>-0.607795</td>\n",
              "      <td>0.118909</td>\n",
              "      <td>-0.756139</td>\n",
              "      <td>0.981187</td>\n",
              "      <td>1.761885</td>\n",
              "      <td>0.817716</td>\n",
              "      <td>-0.414111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>“There is a silence among many who call themselves left.</td>\n",
              "      <td>763260610</td>\n",
              "      <td>9</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>2.433438</td>\n",
              "      <td>-0.736872</td>\n",
              "      <td>0.606246</td>\n",
              "      <td>-0.701917</td>\n",
              "      <td>-0.385232</td>\n",
              "      <td>-0.732447</td>\n",
              "      <td>-0.757679</td>\n",
              "      <td>-0.518542</td>\n",
              "      <td>-0.834346</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.25546</td>\n",
              "      <td>-0.118878</td>\n",
              "      <td>0.565002</td>\n",
              "      <td>0.976212</td>\n",
              "      <td>-0.175996</td>\n",
              "      <td>1.141325</td>\n",
              "      <td>-0.040635</td>\n",
              "      <td>-1.104583</td>\n",
              "      <td>-0.202747</td>\n",
              "      <td>-0.800125</td>\n",
              "      <td>-0.785921</td>\n",
              "      <td>-0.961321</td>\n",
              "      <td>-0.864670</td>\n",
              "      <td>-1.193693</td>\n",
              "      <td>7th and 8th grade</td>\n",
              "      <td>-0.208090</td>\n",
              "      <td>-0.226795</td>\n",
              "      <td>-0.746244</td>\n",
              "      <td>-0.093158</td>\n",
              "      <td>-0.173470</td>\n",
              "      <td>0.218998</td>\n",
              "      <td>-0.177548</td>\n",
              "      <td>-0.048092</td>\n",
              "      <td>-0.095577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.525213</td>\n",
              "      <td>-0.103422</td>\n",
              "      <td>-0.647268</td>\n",
              "      <td>0.353417</td>\n",
              "      <td>-0.613536</td>\n",
              "      <td>-0.036899</td>\n",
              "      <td>-0.045957</td>\n",
              "      <td>-0.374450</td>\n",
              "      <td>-0.559507</td>\n",
              "      <td>0.682506</td>\n",
              "      <td>1.213547</td>\n",
              "      <td>-0.892961</td>\n",
              "      <td>-0.794986</td>\n",
              "      <td>-0.238253</td>\n",
              "      <td>-0.252268</td>\n",
              "      <td>-0.529955</td>\n",
              "      <td>-0.205636</td>\n",
              "      <td>-0.100648</td>\n",
              "      <td>1.391691</td>\n",
              "      <td>-0.726259</td>\n",
              "      <td>-0.350785</td>\n",
              "      <td>-0.444427</td>\n",
              "      <td>-0.768598</td>\n",
              "      <td>-0.288991</td>\n",
              "      <td>-0.414111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>The President of AMANA, Sofian Zakkout, has referred to Duke as “David Duke, a man to believe in!”</td>\n",
              "      <td>728169864</td>\n",
              "      <td>43</td>\n",
              "      <td>propaganda</td>\n",
              "      <td>-1.098724</td>\n",
              "      <td>1.273570</td>\n",
              "      <td>-0.742237</td>\n",
              "      <td>0.886262</td>\n",
              "      <td>-0.206027</td>\n",
              "      <td>-0.180601</td>\n",
              "      <td>-0.285152</td>\n",
              "      <td>-0.518542</td>\n",
              "      <td>-0.834346</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.25546</td>\n",
              "      <td>-0.118878</td>\n",
              "      <td>-1.769904</td>\n",
              "      <td>-0.191990</td>\n",
              "      <td>-0.997068</td>\n",
              "      <td>0.630183</td>\n",
              "      <td>-0.040635</td>\n",
              "      <td>-0.484016</td>\n",
              "      <td>-0.292691</td>\n",
              "      <td>-0.438495</td>\n",
              "      <td>-0.072139</td>\n",
              "      <td>-0.283462</td>\n",
              "      <td>-0.344975</td>\n",
              "      <td>-0.239423</td>\n",
              "      <td>8th and 9th grade</td>\n",
              "      <td>-0.208090</td>\n",
              "      <td>0.089794</td>\n",
              "      <td>2.246304</td>\n",
              "      <td>-0.093158</td>\n",
              "      <td>-0.173470</td>\n",
              "      <td>0.614164</td>\n",
              "      <td>-0.177548</td>\n",
              "      <td>-0.048092</td>\n",
              "      <td>-0.095577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.754521</td>\n",
              "      <td>0.837294</td>\n",
              "      <td>-0.647268</td>\n",
              "      <td>-0.825508</td>\n",
              "      <td>-0.613536</td>\n",
              "      <td>0.126807</td>\n",
              "      <td>-0.045957</td>\n",
              "      <td>-0.374450</td>\n",
              "      <td>0.889610</td>\n",
              "      <td>-0.597938</td>\n",
              "      <td>-0.014715</td>\n",
              "      <td>-0.163144</td>\n",
              "      <td>0.417833</td>\n",
              "      <td>-0.627334</td>\n",
              "      <td>0.444953</td>\n",
              "      <td>1.064789</td>\n",
              "      <td>-0.205636</td>\n",
              "      <td>1.496778</td>\n",
              "      <td>-0.488613</td>\n",
              "      <td>-0.134642</td>\n",
              "      <td>-0.350785</td>\n",
              "      <td>0.981187</td>\n",
              "      <td>1.129264</td>\n",
              "      <td>-0.453238</td>\n",
              "      <td>-0.414111</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>Date of erection: 1994</td>\n",
              "      <td>761334950</td>\n",
              "      <td>82</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>-1.206597</td>\n",
              "      <td>-0.043380</td>\n",
              "      <td>-0.922366</td>\n",
              "      <td>-0.059888</td>\n",
              "      <td>-0.959301</td>\n",
              "      <td>-1.146331</td>\n",
              "      <td>-1.140200</td>\n",
              "      <td>-1.002950</td>\n",
              "      <td>-0.834346</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.25546</td>\n",
              "      <td>-0.118878</td>\n",
              "      <td>-1.769904</td>\n",
              "      <td>-0.191990</td>\n",
              "      <td>-0.997068</td>\n",
              "      <td>1.068261</td>\n",
              "      <td>-0.040635</td>\n",
              "      <td>-1.277765</td>\n",
              "      <td>-0.876657</td>\n",
              "      <td>-1.126759</td>\n",
              "      <td>-0.151788</td>\n",
              "      <td>-0.961321</td>\n",
              "      <td>-1.176487</td>\n",
              "      <td>-0.769049</td>\n",
              "      <td>2nd and 3rd grade</td>\n",
              "      <td>4.782887</td>\n",
              "      <td>0.153752</td>\n",
              "      <td>-0.746244</td>\n",
              "      <td>-0.093158</td>\n",
              "      <td>4.847718</td>\n",
              "      <td>-0.535410</td>\n",
              "      <td>-0.177548</td>\n",
              "      <td>-0.048092</td>\n",
              "      <td>-0.095577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.754521</td>\n",
              "      <td>1.931946</td>\n",
              "      <td>-0.647268</td>\n",
              "      <td>-0.825508</td>\n",
              "      <td>-0.613536</td>\n",
              "      <td>-1.182845</td>\n",
              "      <td>-0.045957</td>\n",
              "      <td>-0.374450</td>\n",
              "      <td>0.646402</td>\n",
              "      <td>-0.597938</td>\n",
              "      <td>-1.652398</td>\n",
              "      <td>-0.428532</td>\n",
              "      <td>-0.537722</td>\n",
              "      <td>0.504539</td>\n",
              "      <td>-0.252268</td>\n",
              "      <td>0.291580</td>\n",
              "      <td>-0.205636</td>\n",
              "      <td>-0.202099</td>\n",
              "      <td>-0.552254</td>\n",
              "      <td>-1.233359</td>\n",
              "      <td>-1.161493</td>\n",
              "      <td>-0.444427</td>\n",
              "      <td>-0.768598</td>\n",
              "      <td>-0.247930</td>\n",
              "      <td>-0.414111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Last year, eight people were infected and four people died after an outbreak of the disease.</td>\n",
              "      <td>754111899</td>\n",
              "      <td>8</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>3.048578</td>\n",
              "      <td>-0.632714</td>\n",
              "      <td>0.130715</td>\n",
              "      <td>-0.737181</td>\n",
              "      <td>-0.699186</td>\n",
              "      <td>-0.318563</td>\n",
              "      <td>-0.352656</td>\n",
              "      <td>-0.518542</td>\n",
              "      <td>-0.834346</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.25546</td>\n",
              "      <td>-0.118878</td>\n",
              "      <td>0.565002</td>\n",
              "      <td>-0.191990</td>\n",
              "      <td>-0.778115</td>\n",
              "      <td>0.431954</td>\n",
              "      <td>-0.040635</td>\n",
              "      <td>-0.426289</td>\n",
              "      <td>0.026812</td>\n",
              "      <td>-0.391833</td>\n",
              "      <td>-0.271262</td>\n",
              "      <td>-0.509415</td>\n",
              "      <td>-0.448914</td>\n",
              "      <td>-0.497748</td>\n",
              "      <td>7th and 8th grade</td>\n",
              "      <td>-0.208090</td>\n",
              "      <td>-0.173265</td>\n",
              "      <td>0.316327</td>\n",
              "      <td>-0.093158</td>\n",
              "      <td>-0.173470</td>\n",
              "      <td>0.291158</td>\n",
              "      <td>-0.177548</td>\n",
              "      <td>-0.048092</td>\n",
              "      <td>-0.095577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.754521</td>\n",
              "      <td>0.182883</td>\n",
              "      <td>0.803977</td>\n",
              "      <td>-0.107901</td>\n",
              "      <td>-0.613536</td>\n",
              "      <td>0.212219</td>\n",
              "      <td>-0.045957</td>\n",
              "      <td>-0.374450</td>\n",
              "      <td>-0.152397</td>\n",
              "      <td>-0.597938</td>\n",
              "      <td>0.092091</td>\n",
              "      <td>-0.004488</td>\n",
              "      <td>-0.589920</td>\n",
              "      <td>-0.309302</td>\n",
              "      <td>-0.252268</td>\n",
              "      <td>-0.737965</td>\n",
              "      <td>-0.205636</td>\n",
              "      <td>-0.527843</td>\n",
              "      <td>-0.112552</td>\n",
              "      <td>-0.303675</td>\n",
              "      <td>-0.080550</td>\n",
              "      <td>-0.444427</td>\n",
              "      <td>-0.768598</td>\n",
              "      <td>-0.196603</td>\n",
              "      <td>-0.414111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>However, since January 2010, the Bureau of Labor Statistics [BLS] has begun to publish figures for foreign-born and native-born employment.</td>\n",
              "      <td>761969038</td>\n",
              "      <td>28</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>-0.194282</td>\n",
              "      <td>-0.418951</td>\n",
              "      <td>0.092696</td>\n",
              "      <td>-0.768592</td>\n",
              "      <td>-0.176013</td>\n",
              "      <td>-0.042640</td>\n",
              "      <td>0.176124</td>\n",
              "      <td>0.450275</td>\n",
              "      <td>0.001906</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.25546</td>\n",
              "      <td>-0.118878</td>\n",
              "      <td>0.565002</td>\n",
              "      <td>-0.191990</td>\n",
              "      <td>-0.997068</td>\n",
              "      <td>-0.997272</td>\n",
              "      <td>-0.040635</td>\n",
              "      <td>0.641663</td>\n",
              "      <td>0.813487</td>\n",
              "      <td>0.494745</td>\n",
              "      <td>0.816260</td>\n",
              "      <td>0.620352</td>\n",
              "      <td>0.382598</td>\n",
              "      <td>0.693615</td>\n",
              "      <td>14th and 15th grade</td>\n",
              "      <td>0.581849</td>\n",
              "      <td>0.221390</td>\n",
              "      <td>0.660325</td>\n",
              "      <td>-0.093158</td>\n",
              "      <td>-0.173470</td>\n",
              "      <td>-0.525279</td>\n",
              "      <td>-0.177548</td>\n",
              "      <td>-0.048092</td>\n",
              "      <td>-0.095577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.754521</td>\n",
              "      <td>0.171349</td>\n",
              "      <td>0.313268</td>\n",
              "      <td>0.124417</td>\n",
              "      <td>-0.132892</td>\n",
              "      <td>-0.721169</td>\n",
              "      <td>-0.045957</td>\n",
              "      <td>-0.374450</td>\n",
              "      <td>0.074833</td>\n",
              "      <td>-0.597938</td>\n",
              "      <td>-0.497772</td>\n",
              "      <td>0.491416</td>\n",
              "      <td>-0.408164</td>\n",
              "      <td>0.494564</td>\n",
              "      <td>0.239298</td>\n",
              "      <td>0.009275</td>\n",
              "      <td>-0.205636</td>\n",
              "      <td>-0.120527</td>\n",
              "      <td>-0.219006</td>\n",
              "      <td>0.118909</td>\n",
              "      <td>-0.350785</td>\n",
              "      <td>0.981187</td>\n",
              "      <td>0.496643</td>\n",
              "      <td>0.778610</td>\n",
              "      <td>-0.414111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>This marks the country’s ninth epidemic since the ebola virus was identified in 1976.</td>\n",
              "      <td>756114837</td>\n",
              "      <td>31</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>1.250405</td>\n",
              "      <td>-0.970534</td>\n",
              "      <td>3.870506</td>\n",
              "      <td>-0.544336</td>\n",
              "      <td>-0.931706</td>\n",
              "      <td>-0.456524</td>\n",
              "      <td>-0.431410</td>\n",
              "      <td>-0.034134</td>\n",
              "      <td>-0.834346</td>\n",
              "      <td>1.007411</td>\n",
              "      <td>1.007411</td>\n",
              "      <td>-0.25546</td>\n",
              "      <td>-0.118878</td>\n",
              "      <td>0.565002</td>\n",
              "      <td>-0.191990</td>\n",
              "      <td>-0.997068</td>\n",
              "      <td>0.494536</td>\n",
              "      <td>-0.040635</td>\n",
              "      <td>-0.541743</td>\n",
              "      <td>0.281877</td>\n",
              "      <td>-0.321840</td>\n",
              "      <td>0.172937</td>\n",
              "      <td>-0.283462</td>\n",
              "      <td>-0.448914</td>\n",
              "      <td>-0.128543</td>\n",
              "      <td>6th and 7th grade</td>\n",
              "      <td>1.083692</td>\n",
              "      <td>-0.310804</td>\n",
              "      <td>-0.746244</td>\n",
              "      <td>-0.093158</td>\n",
              "      <td>-0.173470</td>\n",
              "      <td>-0.021821</td>\n",
              "      <td>-0.177548</td>\n",
              "      <td>-0.048092</td>\n",
              "      <td>-0.095577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.754521</td>\n",
              "      <td>0.314920</td>\n",
              "      <td>-0.647268</td>\n",
              "      <td>0.727899</td>\n",
              "      <td>-0.613536</td>\n",
              "      <td>1.082082</td>\n",
              "      <td>-0.045957</td>\n",
              "      <td>-0.374450</td>\n",
              "      <td>-0.311649</td>\n",
              "      <td>-0.597938</td>\n",
              "      <td>0.865138</td>\n",
              "      <td>-0.912996</td>\n",
              "      <td>-0.495349</td>\n",
              "      <td>-0.001142</td>\n",
              "      <td>-0.252268</td>\n",
              "      <td>-0.711318</td>\n",
              "      <td>-0.205636</td>\n",
              "      <td>0.435856</td>\n",
              "      <td>-0.241876</td>\n",
              "      <td>-0.472709</td>\n",
              "      <td>-0.485903</td>\n",
              "      <td>-0.444427</td>\n",
              "      <td>-0.768598</td>\n",
              "      <td>0.074697</td>\n",
              "      <td>-0.414111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>She also met with Secretary of Defense James Mattis in Texas.</td>\n",
              "      <td>999001297</td>\n",
              "      <td>39</td>\n",
              "      <td>non-propaganda</td>\n",
              "      <td>1.141804</td>\n",
              "      <td>0.030585</td>\n",
              "      <td>-0.375306</td>\n",
              "      <td>0.413049</td>\n",
              "      <td>-0.659696</td>\n",
              "      <td>-0.663466</td>\n",
              "      <td>-0.701425</td>\n",
              "      <td>-1.002950</td>\n",
              "      <td>0.001906</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.409571</td>\n",
              "      <td>-0.25546</td>\n",
              "      <td>-0.118878</td>\n",
              "      <td>0.565002</td>\n",
              "      <td>-0.191990</td>\n",
              "      <td>-0.997068</td>\n",
              "      <td>0.327444</td>\n",
              "      <td>-0.040635</td>\n",
              "      <td>-0.541743</td>\n",
              "      <td>-0.201405</td>\n",
              "      <td>-0.776794</td>\n",
              "      <td>-0.375419</td>\n",
              "      <td>-0.735368</td>\n",
              "      <td>-0.708761</td>\n",
              "      <td>-0.760792</td>\n",
              "      <td>5th and 6th grade</td>\n",
              "      <td>-0.208090</td>\n",
              "      <td>-0.246978</td>\n",
              "      <td>-0.746244</td>\n",
              "      <td>-0.093158</td>\n",
              "      <td>-0.173470</td>\n",
              "      <td>0.318761</td>\n",
              "      <td>-0.177548</td>\n",
              "      <td>-0.048092</td>\n",
              "      <td>-0.095577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.291914</td>\n",
              "      <td>2.206731</td>\n",
              "      <td>-0.647268</td>\n",
              "      <td>-0.825508</td>\n",
              "      <td>0.481703</td>\n",
              "      <td>-1.182845</td>\n",
              "      <td>-0.045957</td>\n",
              "      <td>-0.374450</td>\n",
              "      <td>0.451038</td>\n",
              "      <td>0.577552</td>\n",
              "      <td>-0.775387</td>\n",
              "      <td>-0.250882</td>\n",
              "      <td>-0.006323</td>\n",
              "      <td>-0.336480</td>\n",
              "      <td>-0.252268</td>\n",
              "      <td>1.866519</td>\n",
              "      <td>-0.205636</td>\n",
              "      <td>-0.102441</td>\n",
              "      <td>0.292436</td>\n",
              "      <td>-0.641742</td>\n",
              "      <td>-0.621021</td>\n",
              "      <td>-0.444427</td>\n",
              "      <td>0.812954</td>\n",
              "      <td>-0.341251</td>\n",
              "      <td>-0.414111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>In contrast, President Trump issued a statement claiming that “Ramadan reminds us of the richness Muslims add to the religious tapestry of American life.”</td>\n",
              "      <td>759468687</td>\n",
              "      <td>8</td>\n",
              "      <td>propaganda</td>\n",
              "      <td>0.274326</td>\n",
              "      <td>-0.086448</td>\n",
              "      <td>-0.836827</td>\n",
              "      <td>1.476369</td>\n",
              "      <td>-0.308406</td>\n",
              "      <td>0.233283</td>\n",
              "      <td>0.344884</td>\n",
              "      <td>-0.034134</td>\n",
              "      <td>-0.834346</td>\n",
              "      <td>1.007411</td>\n",
              "      <td>1.007411</td>\n",
              "      <td>-0.25546</td>\n",
              "      <td>-0.118878</td>\n",
              "      <td>-1.769904</td>\n",
              "      <td>-0.191990</td>\n",
              "      <td>-0.586532</td>\n",
              "      <td>-0.600813</td>\n",
              "      <td>-0.040635</td>\n",
              "      <td>0.526209</td>\n",
              "      <td>0.642996</td>\n",
              "      <td>0.436417</td>\n",
              "      <td>1.159365</td>\n",
              "      <td>1.298212</td>\n",
              "      <td>0.382598</td>\n",
              "      <td>1.157185</td>\n",
              "      <td>13th and 14th grade</td>\n",
              "      <td>-0.208090</td>\n",
              "      <td>-0.293950</td>\n",
              "      <td>-0.111461</td>\n",
              "      <td>-0.093158</td>\n",
              "      <td>-0.173470</td>\n",
              "      <td>-0.133059</td>\n",
              "      <td>-0.177548</td>\n",
              "      <td>-0.048092</td>\n",
              "      <td>-0.095577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.340024</td>\n",
              "      <td>0.495216</td>\n",
              "      <td>-0.647268</td>\n",
              "      <td>0.031892</td>\n",
              "      <td>-0.613536</td>\n",
              "      <td>0.067277</td>\n",
              "      <td>-0.045957</td>\n",
              "      <td>-0.374450</td>\n",
              "      <td>0.220787</td>\n",
              "      <td>-0.132322</td>\n",
              "      <td>-0.262849</td>\n",
              "      <td>1.119564</td>\n",
              "      <td>-0.366212</td>\n",
              "      <td>0.108384</td>\n",
              "      <td>-0.252268</td>\n",
              "      <td>0.098278</td>\n",
              "      <td>-0.205636</td>\n",
              "      <td>1.023340</td>\n",
              "      <td>-0.418360</td>\n",
              "      <td>0.287942</td>\n",
              "      <td>-0.080550</td>\n",
              "      <td>-0.444427</td>\n",
              "      <td>0.812954</td>\n",
              "      <td>0.333776</td>\n",
              "      <td>-0.414111</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                 sentences  \\\n",
              "31  The Nuncio added that Littleton had already forwarded his Memorandum to about twenty people, including civil and ecclesiastical judicial authorities, police and lawyers, in June 2006, and that it was therefore very likely that the news would soon be made public.   \n",
              "21                                                                                                                     FACT asked the Senate ethics committee to probe fundraising emails sent by Ms. Warren, Massachusetts Democrat, and Ms. Harris, California Democrat.   \n",
              "8                                                                                                                                                                                                                 “There is a silence among many who call themselves left.   \n",
              "42                                                                                                                                                                      The President of AMANA, Sofian Zakkout, has referred to Duke as “David Duke, a man to believe in!”   \n",
              "81                                                                                                                                                                                                                                                  Date of erection: 1994   \n",
              "7                                                                                                                                                                             Last year, eight people were infected and four people died after an outbreak of the disease.   \n",
              "27                                                                                                                             However, since January 2010, the Bureau of Labor Statistics [BLS] has begun to publish figures for foreign-born and native-born employment.   \n",
              "30                                                                                                                                                                                   This marks the country’s ninth epidemic since the ebola virus was identified in 1976.   \n",
              "38                                                                                                                                                                                                           She also met with Secretary of Defense James Mattis in Texas.   \n",
              "7                                                                                                               In contrast, President Trump issued a statement claiming that “Ramadan reminds us of the richness Muslims add to the religious tapestry of American life.”   \n",
              "\n",
              "      article  N_sentence   is_propaganda   sadness       joy      fear  \\\n",
              "31  782086447          32  non-propaganda  0.235276 -0.911798  0.848839   \n",
              "21  999000147          22  non-propaganda  0.743346 -0.960339 -0.507128   \n",
              "8   763260610           9  non-propaganda  2.433438 -0.736872  0.606246   \n",
              "42  728169864          43      propaganda -1.098724  1.273570 -0.742237   \n",
              "81  761334950          82  non-propaganda -1.206597 -0.043380 -0.922366   \n",
              "7   754111899           8  non-propaganda  3.048578 -0.632714  0.130715   \n",
              "27  761969038          28  non-propaganda -0.194282 -0.418951  0.092696   \n",
              "30  756114837          31  non-propaganda  1.250405 -0.970534  3.870506   \n",
              "38  999001297          39  non-propaganda  1.141804  0.030585 -0.375306   \n",
              "7   759468687           8      propaganda  0.274326 -0.086448 -0.836827   \n",
              "\n",
              "     disgust     anger  num_words  num_chars  num_adjs   num_adv  sing_pro  \\\n",
              "31  0.053167 -0.723438   1.405955   1.559952  1.903499  2.510659 -0.409571   \n",
              "21  1.215750  0.886222   0.026341   0.266129 -0.518542 -0.834346 -0.409571   \n",
              "8  -0.701917 -0.385232  -0.732447  -0.757679 -0.518542 -0.834346 -0.409571   \n",
              "42  0.886262 -0.206027  -0.180601  -0.285152 -0.518542 -0.834346 -0.409571   \n",
              "81 -0.059888 -0.959301  -1.146331  -1.140200 -1.002950 -0.834346 -0.409571   \n",
              "7  -0.737181 -0.699186  -0.318563  -0.352656 -0.518542 -0.834346 -0.409571   \n",
              "27 -0.768592 -0.176013  -0.042640   0.176124  0.450275  0.001906 -0.409571   \n",
              "30 -0.544336 -0.931706  -0.456524  -0.431410 -0.034134 -0.834346  1.007411   \n",
              "38  0.413049 -0.659696  -0.663466  -0.701425 -1.002950  0.001906 -0.409571   \n",
              "7   1.476369 -0.308406   0.233283   0.344884 -0.034134 -0.834346  1.007411   \n",
              "\n",
              "      pl_pro  is_question  is_exclamation  is_period  polarity  subjectivity  \\\n",
              "31 -0.409571     -0.25546       -0.118878   0.565002 -0.191990      0.754552   \n",
              "21 -0.409571     -0.25546       -0.118878   0.565002 -0.191990     -0.997068   \n",
              "8  -0.409571     -0.25546       -0.118878   0.565002  0.976212     -0.175996   \n",
              "42 -0.409571     -0.25546       -0.118878  -1.769904 -0.191990     -0.997068   \n",
              "81 -0.409571     -0.25546       -0.118878  -1.769904 -0.191990     -0.997068   \n",
              "7  -0.409571     -0.25546       -0.118878   0.565002 -0.191990     -0.778115   \n",
              "27 -0.409571     -0.25546       -0.118878   0.565002 -0.191990     -0.997068   \n",
              "30  1.007411     -0.25546       -0.118878   0.565002 -0.191990     -0.997068   \n",
              "38 -0.409571     -0.25546       -0.118878   0.565002 -0.191990     -0.997068   \n",
              "7   1.007411     -0.25546       -0.118878  -1.769904 -0.191990     -0.586532   \n",
              "\n",
              "    flesch_reading_ease  smog_index  flesch_kincaid_grade  coleman_liau_index  \\\n",
              "31            -1.132919   -0.040635              1.493139            0.652393   \n",
              "21            -0.069015   17.823058             -0.426289            0.540970   \n",
              "8              1.141325   -0.040635             -1.104583           -0.202747   \n",
              "42             0.630183   -0.040635             -0.484016           -0.292691   \n",
              "81             1.068261   -0.040635             -1.277765           -0.876657   \n",
              "7              0.431954   -0.040635             -0.426289            0.026812   \n",
              "27            -0.997272   -0.040635              0.641663            0.813487   \n",
              "30             0.494536   -0.040635             -0.541743            0.281877   \n",
              "38             0.327444   -0.040635             -0.541743           -0.201405   \n",
              "7             -0.600813   -0.040635              0.526209            0.642996   \n",
              "\n",
              "    automated_readability_index  dale_chall_readability_score  \\\n",
              "31                     1.404653                      0.381251   \n",
              "21                    -0.228516                      0.528296   \n",
              "8                     -0.800125                     -0.785921   \n",
              "42                    -0.438495                     -0.072139   \n",
              "81                    -1.126759                     -0.151788   \n",
              "7                     -0.391833                     -0.271262   \n",
              "27                     0.494745                      0.816260   \n",
              "30                    -0.321840                      0.172937   \n",
              "38                    -0.776794                     -0.375419   \n",
              "7                      0.436417                      1.159365   \n",
              "\n",
              "    difficult_words  linsear_write_formula  gunning_fog        text_standard  \\\n",
              "31         1.072258               1.577896     0.948402  20th and 21st grade   \n",
              "21         0.620352              -0.916639    -0.009407   9th and 10th grade   \n",
              "8         -0.961321              -0.864670    -1.193693    7th and 8th grade   \n",
              "42        -0.283462              -0.344975    -0.239423    8th and 9th grade   \n",
              "81        -0.961321              -1.176487    -0.769049    2nd and 3rd grade   \n",
              "7         -0.509415              -0.448914    -0.497748    7th and 8th grade   \n",
              "27         0.620352               0.382598     0.693615  14th and 15th grade   \n",
              "30        -0.283462              -0.448914    -0.128543    6th and 7th grade   \n",
              "38        -0.735368              -0.708761    -0.760792    5th and 6th grade   \n",
              "7          1.298212               0.382598     1.157185  13th and 14th grade   \n",
              "\n",
              "      digits  punctuation_count  comma_count  semicolon_count  colon_count  \\\n",
              "31  0.211000          -0.209878     0.746223        -0.093158    -0.173470   \n",
              "21 -0.208090           0.089794     1.248788        -0.093158    -0.173470   \n",
              "8  -0.208090          -0.226795    -0.746244        -0.093158    -0.173470   \n",
              "42 -0.208090           0.089794     2.246304        -0.093158    -0.173470   \n",
              "81  4.782887           0.153752    -0.746244        -0.093158     4.847718   \n",
              "7  -0.208090          -0.173265     0.316327        -0.093158    -0.173470   \n",
              "27  0.581849           0.221390     0.660325        -0.093158    -0.173470   \n",
              "30  1.083692          -0.310804    -0.746244        -0.093158    -0.173470   \n",
              "38 -0.208090          -0.246978    -0.746244        -0.093158    -0.173470   \n",
              "7  -0.208090          -0.293950    -0.111461        -0.093158    -0.173470   \n",
              "\n",
              "    spaces_count  apostrophes_count  amp_count  parenthesis_count  \\\n",
              "31     -0.030168          -0.177548  -0.048092          -0.095577   \n",
              "21     -0.544990          -0.177548  -0.048092          -0.095577   \n",
              "8       0.218998          -0.177548  -0.048092          -0.095577   \n",
              "42      0.614164          -0.177548  -0.048092          -0.095577   \n",
              "81     -0.535410          -0.177548  -0.048092          -0.095577   \n",
              "7       0.291158          -0.177548  -0.048092          -0.095577   \n",
              "27     -0.525279          -0.177548  -0.048092          -0.095577   \n",
              "30     -0.021821          -0.177548  -0.048092          -0.095577   \n",
              "38      0.318761          -0.177548  -0.048092          -0.095577   \n",
              "7      -0.133059          -0.177548  -0.048092          -0.095577   \n",
              "\n",
              "    paragraph_count  pronouns  prepositions  coordinating_conjunctions  \\\n",
              "31              0.0 -0.267249     -0.012941                   0.881524   \n",
              "21              0.0 -0.754521     -0.918709                   0.260994   \n",
              "8               0.0  1.525213     -0.103422                  -0.647268   \n",
              "42              0.0 -0.754521      0.837294                  -0.647268   \n",
              "81              0.0 -0.754521      1.931946                  -0.647268   \n",
              "7               0.0 -0.754521      0.182883                   0.803977   \n",
              "27              0.0 -0.754521      0.171349                   0.313268   \n",
              "30              0.0 -0.754521      0.314920                  -0.647268   \n",
              "38              0.0  0.291914      2.206731                  -0.647268   \n",
              "7               0.0 -0.340024      0.495216                  -0.647268   \n",
              "\n",
              "    adjectives   adverbs  determiners  interjections    modals     nouns  \\\n",
              "31    0.686396  0.406458    -0.692975      -0.045957  0.227527 -0.589289   \n",
              "21   -0.825508 -0.613536    -0.746294      -0.045957 -0.374450  0.592356   \n",
              "8     0.353417 -0.613536    -0.036899      -0.045957 -0.374450 -0.559507   \n",
              "42   -0.825508 -0.613536     0.126807      -0.045957 -0.374450  0.889610   \n",
              "81   -0.825508 -0.613536    -1.182845      -0.045957 -0.374450  0.646402   \n",
              "7    -0.107901 -0.613536     0.212219      -0.045957 -0.374450 -0.152397   \n",
              "27    0.124417 -0.132892    -0.721169      -0.045957 -0.374450  0.074833   \n",
              "30    0.727899 -0.613536     1.082082      -0.045957 -0.374450 -0.311649   \n",
              "38   -0.825508  0.481703    -1.182845      -0.045957 -0.374450  0.451038   \n",
              "7     0.031892 -0.613536     0.067277      -0.045957 -0.374450  0.220787   \n",
              "\n",
              "    personal_pronouns     verbs  word_len_gte_six  word_len_two_and_three  \\\n",
              "31          -0.324255 -0.223071          0.315972               -0.528557   \n",
              "21          -0.597938 -0.196680          0.972127               -0.660229   \n",
              "8            0.682506  1.213547         -0.892961               -0.794986   \n",
              "42          -0.597938 -0.014715         -0.163144                0.417833   \n",
              "81          -0.597938 -1.652398         -0.428532               -0.537722   \n",
              "7           -0.597938  0.092091         -0.004488               -0.589920   \n",
              "27          -0.597938 -0.497772          0.491416               -0.408164   \n",
              "30          -0.597938  0.865138         -0.912996               -0.495349   \n",
              "38           0.577552 -0.775387         -0.250882               -0.006323   \n",
              "7           -0.132322 -0.262849          1.119564               -0.366212   \n",
              "\n",
              "    avg_word_length  all_caps  capitalized  quotes_count  sentiment_score  \\\n",
              "31         0.007077 -0.252268    -0.493434     -0.205636         0.363518   \n",
              "21         0.513972  0.212546     0.761029     -0.205636         0.475084   \n",
              "8         -0.238253 -0.252268    -0.529955     -0.205636        -0.100648   \n",
              "42        -0.627334  0.444953     1.064789     -0.205636         1.496778   \n",
              "81         0.504539 -0.252268     0.291580     -0.205636        -0.202099   \n",
              "7         -0.309302 -0.252268    -0.737965     -0.205636        -0.527843   \n",
              "27         0.494564  0.239298     0.009275     -0.205636        -0.120527   \n",
              "30        -0.001142 -0.252268    -0.711318     -0.205636         0.435856   \n",
              "38        -0.336480 -0.252268     1.866519     -0.205636        -0.102441   \n",
              "7          0.108384 -0.252268     0.098278     -0.205636         1.023340   \n",
              "\n",
              "    normalized_meanings  num_unique_words  num_stop_words  num_words_upper  \\\n",
              "31             0.199365          1.555694        1.675983        -0.444427   \n",
              "21            -0.607795          0.118909       -0.756139         0.981187   \n",
              "8              1.391691         -0.726259       -0.350785        -0.444427   \n",
              "42            -0.488613         -0.134642       -0.350785         0.981187   \n",
              "81            -0.552254         -1.233359       -1.161493        -0.444427   \n",
              "7             -0.112552         -0.303675       -0.080550        -0.444427   \n",
              "27            -0.219006          0.118909       -0.350785         0.981187   \n",
              "30            -0.241876         -0.472709       -0.485903        -0.444427   \n",
              "38             0.292436         -0.641742       -0.621021        -0.444427   \n",
              "7             -0.418360          0.287942       -0.080550        -0.444427   \n",
              "\n",
              "    num_words_title  mean_word_len       llc  target  \n",
              "31         0.496643       0.297889 -0.414111       0  \n",
              "21         1.761885       0.817716 -0.414111       0  \n",
              "8         -0.768598      -0.288991 -0.414111       0  \n",
              "42         1.129264      -0.453238 -0.414111       1  \n",
              "81        -0.768598      -0.247930 -0.414111       0  \n",
              "7         -0.768598      -0.196603 -0.414111       0  \n",
              "27         0.496643       0.778610 -0.414111       0  \n",
              "30        -0.768598       0.074697 -0.414111       0  \n",
              "38         0.812954      -0.341251 -0.414111       0  \n",
              "7          0.812954       0.333776 -0.414111       1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "metadata": {
        "id": "gmy1lBJhM5ea",
        "colab_type": "code",
        "outputId": "06d541ed-6773-4ff9-836d-e2425dfdbc3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        }
      },
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train[['gunning_fog']] = scaler.fit_transform(X_train[['gunning_fog']])\n",
        "X_val[['gunning_fog']] = scaler.transform(X_val[['gunning_fog']])\n",
        "X_test[['gunning_fog']] = scaler.transform(X_test[['gunning_fog']])|"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self.obj[item] = s\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self.obj[item] = s\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self.obj[item] = s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "gF-MMEtzOHki",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'num_words', 'num_chars', 'num_adjs',\n",
        "       'num_adv', 'sing_pro', 'pl_pro', 'is_question', 'is_exclamation',\n",
        "       'is_period', 'polarity', 'subjectivity', 'flesch_reading_ease',\n",
        "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
        "       'automated_readability_index', 'dale_chall_readability_score',\n",
        "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
        "       'digits', 'punctuation_count', 'comma_count',\n",
        "       'semicolon_count', 'colon_count', 'spaces_count', 'apostrophes_count',\n",
        "       'amp_count', 'parenthesis_count', 'paragraph_count', 'pronouns',\n",
        "       'prepositions', 'coordinating_conjunctions', 'adjectives', 'adverbs',\n",
        "       'determiners', 'interjections', 'modals', 'nouns', 'personal_pronouns',\n",
        "       'verbs', 'word_len_gte_six', 'word_len_two_and_three',\n",
        "       'avg_word_length', 'all_caps', 'capitalized', 'quotes_count',\n",
        "       'sentiment_score', 'normalized_meanings', 'num_unique_words',\n",
        "       'num_stop_words', 'num_words_upper', 'num_words_title', 'mean_word_len',\n",
        "       'llc']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uk046ZOFUDa7",
        "colab_type": "code",
        "outputId": "d3056642-b7c3-4d3c-d358-9436d046d929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "cell_type": "code",
      "source": [
        "X_train[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'num_words', 'num_chars', 'num_adjs',\n",
        "       'num_adv', 'sing_pro', 'pl_pro', 'is_question', 'is_exclamation',\n",
        "       'is_period', 'polarity', 'subjectivity', 'flesch_reading_ease',\n",
        "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
        "       'automated_readability_index', 'dale_chall_readability_score',\n",
        "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
        "       'digits', 'punctuation_count', 'comma_count',\n",
        "       'semicolon_count', 'colon_count', 'spaces_count', 'apostrophes_count',\n",
        "       'amp_count', 'parenthesis_count', 'paragraph_count', 'pronouns',\n",
        "       'prepositions', 'coordinating_conjunctions', 'adjectives', 'adverbs',\n",
        "       'determiners', 'interjections', 'modals', 'nouns', 'personal_pronouns',\n",
        "       'verbs', 'word_len_gte_six', 'word_len_two_and_three',\n",
        "       'avg_word_length', 'all_caps', 'capitalized', 'quotes_count',\n",
        "       'sentiment_score', 'normalized_meanings', 'num_unique_words',\n",
        "       'num_stop_words', 'num_words_upper', 'num_words_title', 'mean_word_len',\n",
        "       'llc']] = scaler.fit_transform(X_train[['sadness', 'joy',\n",
        "       'fear', 'disgust', 'anger', 'num_words', 'num_chars', 'num_adjs',\n",
        "       'num_adv', 'sing_pro', 'pl_pro', 'is_question', 'is_exclamation',\n",
        "       'is_period', 'polarity', 'subjectivity', 'flesch_reading_ease',\n",
        "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
        "       'automated_readability_index', 'dale_chall_readability_score',\n",
        "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
        "       'digits', 'punctuation_count', 'comma_count',\n",
        "       'semicolon_count', 'colon_count', 'spaces_count', 'apostrophes_count',\n",
        "       'amp_count', 'parenthesis_count', 'paragraph_count', 'pronouns',\n",
        "       'prepositions', 'coordinating_conjunctions', 'adjectives', 'adverbs',\n",
        "       'determiners', 'interjections', 'modals', 'nouns', 'personal_pronouns',\n",
        "       'verbs', 'word_len_gte_six', 'word_len_two_and_three',\n",
        "       'avg_word_length', 'all_caps', 'capitalized', 'quotes_count',\n",
        "       'sentiment_score', 'normalized_meanings', 'num_unique_words',\n",
        "       'num_stop_words', 'num_words_upper', 'num_words_title', 'mean_word_len',\n",
        "       'llc']])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, **fit_params).transform(X)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self.obj[item] = s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rqylk_3dUOTj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}